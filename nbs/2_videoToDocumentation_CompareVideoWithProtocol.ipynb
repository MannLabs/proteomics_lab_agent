{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation assistant\n",
    "\n",
    "This notebook demonstrates a documentation assistant: Video-to-documentation conversion using Vertex AI\n",
    "\n",
    "Converting videos-to-documentation involves three steps: \n",
    "1. Protocol finder: Select protocol which best captures the step being performed in the video\n",
    "2. Video comparing to ground-of-truth protocol → lab documentation + errors in procedure\n",
    "3. Analytics based on benchmark dataset: automatic comparison of errors found by documentation assistent vs actual errors\n",
    "\n",
    "In this notebook, I will focus on the step two and three - Compare video with protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../secrets.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_to_append = Path(Path.cwd()).parent / \"proteomics_specialist\"\n",
    "sys.path.append(str(path_to_append))\n",
    "import video_to_protocol\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")\n",
    "\n",
    "PROJECT_ID = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "vertexai.init(project=PROJECT_ID, location=\"europe-west9\")  # europe-west9 is Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "\n",
    "# Initialize Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"mannlab_videos\"\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "\n",
    "upload_docu = [\n",
    "    \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation/TimsCalibration_docuSavedMethod.mov\",\n",
    "    \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/TimsCalibration_protocolCorrect.md\",\n",
    "    \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/TimsCalibration_docuSavedMethod.md\"\n",
    "    ]\n",
    "for file in upload_docu:\n",
    "    video_to_protocol.upload_video_to_gcs(file, bucket, \"compare_protocol_video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "\n",
    "# Example\n",
    "video_path = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation/TimsCalibration_docuSavedMethod.mov\"\n",
    "video_uri = video_to_protocol.upload_video_to_gcs(video_path, bucket, \"compare_protocol_video\")\n",
    "video_input = [\n",
    "    \"## Lab video:\",\n",
    "    Part.from_uri(video_uri, mime_type=\"video/mp4\"),\n",
    "]\n",
    "\n",
    "path = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/TimsCalibration_protocolCorrect.md\"\n",
    "uri = video_to_protocol.upload_video_to_gcs(path, bucket, \"compare_protocol_video\")\n",
    "protocol_input = [\n",
    "    \"## Protocol:\",\n",
    "    Part.from_uri(uri, mime_type=\"text/md\"),\n",
    "]\n",
    "\n",
    "path = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/TimsCalibration_docuSavedMethod.md\"\n",
    "uri = video_to_protocol.upload_video_to_gcs(path, bucket, \"compare_protocol_video\")\n",
    "documentation_input = [\n",
    "    \"## Documentation:\",\n",
    "    Part.from_uri(uri, mime_type=\"text/md\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "video_path = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation/QueueSamples_docuWrongRow_S3A1Twice.mov\"\n",
    "video_uri = video_to_protocol.upload_video_to_gcs(video_path, bucket, \"compare_protocol_video\")\n",
    "video_input2 = [\n",
    "    \"## Lab video:\",\n",
    "    Part.from_uri(video_uri, mime_type=\"video/mp4\"),\n",
    "]\n",
    "\n",
    "path = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/QueueSamples_protocolCorrect.md\"\n",
    "uri = video_to_protocol.upload_video_to_gcs(path, bucket, \"compare_protocol_video\")\n",
    "protocol_input2 = [\n",
    "    \"## Protocol:\",\n",
    "    Part.from_uri(uri, mime_type=\"text/md\"),\n",
    "]\n",
    "\n",
    "path = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data/QueueSamples_docuWrongRow_S3A1Twice.md\"\n",
    "uri = video_to_protocol.upload_video_to_gcs(path, bucket, \"compare_protocol_video\")\n",
    "documentation_input2 = [\n",
    "    \"## Documentation:\",\n",
    "    Part.from_uri(uri, mime_type=\"text/md\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation/QueueSamples_docuWrongRow_S3A1Twice.mov\n",
      "prompt_token_count: 53314\n",
      "candidates_token_count: 647\n",
      "total_token_count: 53961\n",
      "prompt_tokens_details {\n",
      "  modality: AUDIO\n",
      "  token_count: 4425\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: VIDEO\n",
      "  token_count: 46020\n",
      "}\n",
      "prompt_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 2869\n",
      "}\n",
      "candidates_tokens_details {\n",
      "  modality: TEXT\n",
      "  token_count: 647\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, Professor Mann, here's the corrected documentation based on the provided video, reflecting the actual actions performed and their timing.\n",
       "\n",
       "## Documentation:# Queue and measure samples in HyStar\n",
       "\n",
       "## Aim\n",
       "Queueing samples in HyStar for LC-MS measurement.\n",
       "\n",
       "## Materials\n",
       "\n",
       "### Software\n",
       "HyStar 6.0\n",
       "\n",
       "## Procedure\n",
       "Timing: 3 minutes\n",
       "\n",
       "Prerequisite 1. ✓ Mentioned that 5 ng HeLa Evotips were placed at S1 from A1 to A6 and blanks at S3 from A1 to A6.\n",
       "\n",
       "Prerequisite 2. ✓ Reported that the TIMS device had already been calibrated.\n",
       "\n",
       "1. ✓ Navigated to the 'Acquisition' tab in HyStar.\n",
       "2. ✓ Selected an already existing sample table by pressing the arrow down button when hovering over the sample table name in the left sample table column.\n",
       "\n",
       "3. ❌ **Omitted:** Copied already existing sample table entries to modify them\n",
       "4. ⚠️ **Deviation:** Manually Adjusted the sample ID without following this pattern: currentDate_massSpec_user_sampleType_projectID_ sampleName, instead created \"THMS50tcep_PAlk_SA_blank\".\n",
       "\n",
       "5. ⚠️ **Deviation:** The queue did not contain three dda-PASEF or three dia-PASEF runs. The queue consisted of multiple rows with samples labelled \"THMS50tcep_PAlk_SA_blank\" and \"THMS50tcep_PAlk_MA_HeLa\"\n",
       "    \n",
       "6. ✓ Verified the column autocompletion settings with right-click on a field in the column 'vial'. The arrows pointet from A1-A12, indicating that values increased to the right. The tray type was set to 'Evosep' and slots 1-6 were designated as '96Evotip'.\n",
       "\n",
       "7. ✓ Matched the Evotip position with the sample's location in the Evotip box. The first Evotip was placed in position S1 A1, and all remaining positions were specified individually and automatically by dragging the values.\n",
       "8. ⚠️ **Deviation:** Path was not explicity specified.\n",
       "9. ✓ The separation method \"WhisperRj.zoom\" was selected.\n",
       "\n",
       "10. ✓ Injection method was set to 'standard'.\n",
       "\n",
       "11. ✓ The MS method \"20240703_DDA_maintenance_ionOptics_100ms_m/z713_300-1200_HS_1800V\" was selected.\n",
       "\n",
       "12. ❌ **Omitted:** Idle flow on the Evosep was not canceled.\n",
       "13. ✓ Saved the sample table.\n",
       "\n",
       "14. ⚠️ **Deviation:** Only the last row was selected to upload sample conditions, instead of all rows. The status changed to loaded.\n",
       "\n",
       "15. ✓ Pressed 'start' and the sequence started to run.\n",
       "\n",
       "## Expected Results\n",
       "- ✓ The sample table was running.\n",
       "\n",
       "## Figures\n",
       "\n",
       "### Figure 1: Hystar\n",
       "- Screenshot of Hystar settings\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "inputs = [\n",
    "    \"\"\"\n",
    "    You are Professor Matthias Mann, a pioneering scientist in proteomics and mass spectrometry.\n",
    "    \n",
    "    # Your Task:\n",
    "    Compare the original protocol with the actual implementation shown in a video, and create a corrected documentation that reflects what actually happened. \n",
    "    \n",
    "    Your documentation should follow these guidelines:\n",
    "    1. Keep only relevant sections: Aim, Materials, Procedure, Results\n",
    "    2. Use past tense to describe what actually happened, not what should happen\n",
    "    3. Remove all instructional language and replace with observations\n",
    "    4. Maintain step numbering of the original protocol even if the order is changed (1, 3, 2, ...)\n",
    "    5. Include exact actual timing, not estimated timing\n",
    "\n",
    "    Use these consistent symbols to indicate step status:\n",
    "    - ✓ (Followed correctly with no special notation needed)\n",
    "    - ❌ **Error:** (When something was done incorrectly - be specific about what happened)\n",
    "    - ❌ **Omitted:** (When a step was completely skipped)\n",
    "    - ⚠️ **Deviation:** (When a step was followed differently than prescribed)\n",
    "    - ➕ **Added:** (When a new step not in the protocol was performed)\n",
    "\n",
    "    # Example\n",
    "    \"\"\"\n",
    "]\n",
    "inputs.extend(video_input)\n",
    "inputs.extend(protocol_input)\n",
    "inputs.extend(documentation_input)\n",
    "\n",
    "inputs.extend(\n",
    "    [\"\"\"\n",
    "    # Your task now\n",
    "    Provide me with a documentation as in the example above.\n",
    "    \"\"\"]\n",
    ")\n",
    "\n",
    "inputs.extend(video_input2)\n",
    "inputs.extend(protocol_input2)\n",
    "\n",
    "inputs.append(\n",
    "    \"Output: Correct documentation\"\n",
    ")\n",
    "\n",
    "model = GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    inputs, \n",
    "    generation_config=GenerationConfig(\n",
    "        temperature=0.9,\n",
    "        # audio_timestamp=True # Supported if only one video or audio is used\n",
    "    )\n",
    ")\n",
    "\n",
    "print(video_path)\n",
    "documentation = response.text\n",
    "print(response.usage_metadata)\n",
    "Markdown(documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Part 1: Error Identification Accuracy\n",
       "\n",
       "| Step | Benchmark | AI Response | Classification |\n",
       "|------|-----------|-------------|----------------|\n",
       "| Prerequisite 1 | No Error | No Error | No Error |\n",
       "| Prerequisite 2 | No Error | No Error | No Error |\n",
       "| 1 | No Error | No Error | No Error |\n",
       "| 2 | No Error | No Error | No Error |\n",
       "| 3 | No Error | Error | False Positive |\n",
       "| 4 | No Error | Error | False Positive |\n",
       "| 5 | No Error | Error | False Positive |\n",
       "| 6 | No Error | No Error | No Error |\n",
       "| 7 | Error | No Error | False Negative |\n",
       "| 8 | No Error | Error | False Positive |\n",
       "| 9 | No Error | No Error | No Error |\n",
       "| 10 | No Error | No Error | No Error |\n",
       "| 11 | No Error | No Error | No Error |\n",
       "| 12 | Error | Error | Error (Correctly Identified) |\n",
       "| 13 | No Error | No Error | No Error |\n",
       "| 14 | No Error | Error | False Positive |\n",
       "| 15 | No Error | No Error | No Error |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correct identifications: 11/16\n",
       "- Total false positives: 6\n",
       "- Total false negatives: 1\n",
       "- Overall accuracy: 68.75%\n",
       "\n",
       "## Part 2: Error Classification Accuracy\n",
       "\n",
       "| Step | Benchmark Error Type | AI Error Type | Classification |\n",
       "|------|---------------------|---------------|----------------|\n",
       "| 12 | Omitted | Omitted | Correct |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correctly classified errors: 1/1\n",
       "- Error classification accuracy: 100%\n",
       "\n",
       "## Part 3: Documentation Quality\n",
       "\n",
       "| Criterion | Rating | Explanation |\n",
       "|-----------|--------|-------------|\n",
       "| Structure | Good | The AI kept the relevant sections (Aim, Materials, Procedure, Results) but also included \"Expected Results\" and \"Figures\", which are not typically part of a post-experiment documentation. |\n",
       "| Tense | Poor | The AI continues to use instructional language instead of describing what happened. |\n",
       "| Language | Poor | The AI uses instructional language and includes unnecessary conversational elements (\"Okay, Professor Mann\"). It uses checkmarks and the words \"Mentioned\" and \"Reported,\" which are inappropriate for documentation. |\n",
       "| Numbering | Excellent | The AI maintained the step numbering of the original protocol. |\n",
       "| Timing | Poor | The AI provided an estimated timing rather than an exact actual timing. |\n",
       "\n",
       "## Overall Assessment\n",
       "\n",
       "The AI-generated documentation has significant shortcomings. While it maintains the original numbering and identifies one error correctly, it struggles with differentiating between a protocol and documentation, leading to inappropriate language and tense usage. The high number of false positives significantly reduces its accuracy. The documentation also fails to incorporate actual timings.\n",
       "\n",
       "**Recommendations for Improvement:**\n",
       "\n",
       "1.  **Focus on Tense and Language:** Train the AI to strictly use past tense and remove any instructional or conversational language. The output should read as a record of what *was* done, not what *should* be done.\n",
       "2.  **Reduce False Positives:** Improve the AI's ability to distinguish between minor deviations and actual errors. It needs to be more precise in identifying when a step deviates significantly from the original protocol.\n",
       "3.  **Adhere to Structure:** Stick to the core sections (Aim, Materials, Procedure, Results) without adding speculative sections like \"Expected Results\".\n",
       "4.  **Incorporate Timing:** The AI needs to be able to extract and include exact timings from the video, rather than providing estimated times.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\n",
    "  \"\"\"\n",
    "  # Instruction\n",
    "  You are an expert evaluator specializing in scientific protocol documentation. Your task is to evaluate the error identification accuracy, error type classification and documentation quality of an AI-generated documentation against a benchmark documentation (ground truth). You will be provided with an AI-generated documentation and a benchmark documentation (human-verified ground truth).\n",
    "\n",
    "  # Evaluation Parts\n",
    "  ## Part 1: Error Identification Accuracy\n",
    "  For each step in the protocol, determine if the AI correctly identified the presence or absence of errors by classifying into one of these categories:\n",
    "  - **No Error**: Both benchmark and AI response agree there was no error\n",
    "  - **Error (Correctly Identified)**: Both benchmark and AI response agree there was an error\n",
    "  - **False Positive**: AI response claimed an error when the benchmark indicates none\n",
    "  - **False Negative**: AI response missed an error that the benchmark shows\n",
    "\n",
    "  ## Part 2: Error Type Classification\n",
    "  For each error that was correctly identified by both the benchmark and AI response, determine if the AI correctly classified the error type:\n",
    "  - **Correct Classification**: AI used the same error type as the benchmark (Omitted, Error, Deviation, Added)\n",
    "  - **Incorrect Classification**: AI used a different error type than the benchmark\n",
    "\n",
    "  ## Part 3: Documentation Quality\n",
    "  Evaluate the AI's documentation quality based on these criteria:\n",
    "  1. **Structure**: Did it keep only relevant sections: Aim, Materials, Procedure, Results?\n",
    "  2. **Tense**: Did it use past tense to describe what actually happened, not what should happen?\n",
    "  3. **Language**: Did it remove all instructional language and replace with observations?\n",
    "  4. **Numbering**: Did it maintain step numbering of the original protocol even if order changed?\n",
    "  5. **Timing**: Did it include exact actual timing, not estimated timing?\n",
    "\n",
    "  # Rating Rubric\n",
    "  For each part, provide an evaluation:\n",
    "\n",
    "  ### Part 1: Error Identification Accuracy\n",
    "  - Calculate and report:\n",
    "    - Total number of correct identifications (No Error + Correctly Identified Error)\n",
    "    - Total number of false positives\n",
    "    - Total number of false negatives\n",
    "    - Overall accuracy percentage (correct identifications / total steps)\n",
    "\n",
    "  ### Part 2: Error Type Classification\n",
    "  - Calculate and report:\n",
    "    - Total errors correctly classified / Total errors correctly identified\n",
    "    - Overall error classification accuracy percentage\n",
    "\n",
    "  ### Part 3: Documentation Quality\n",
    "  For each criterion:\n",
    "  - **Excellent**: The criterion was fully met with no issues\n",
    "  - **Good**: The criterion was mostly met with minor issues\n",
    "  - **Poor**: The criterion was not met or had significant issues\n",
    "\n",
    "  # Evaluation Steps\n",
    "  1. Create a table for each step in the protocol showing error identification accuracy\n",
    "  2. Analyze correctly identified errors to determine classification accuracy\n",
    "  3. Evaluate documentation quality against the 5 criteria\n",
    "  4. Provide final scores and overall assessment\n",
    "  5. Highlight specific strengths and areas for improvement\n",
    "\n",
    "  # Output Format\n",
    "  ## Part 1: Error Identification Accuracy\n",
    "  | Step | Benchmark | AI Response | Classification |\n",
    "  |------|-----------|-------------|----------------|\n",
    "  | [Step details] | [Error/No Error] | [Error/No Error] | [No Error/Error/False Positive/False Negative] |\n",
    "\n",
    "  **Summary Statistics:**\n",
    "  - Total correct identifications: [X]/[Total Steps]\n",
    "  - Total false positives: [X]\n",
    "  - Total false negatives: [X]\n",
    "  - Overall accuracy: [X]%\n",
    "\n",
    "  ## Part 2: Error Classification Accuracy\n",
    "  | Step | Benchmark Error Type | AI Error Type | Classification |\n",
    "  |------|---------------------|---------------|----------------|\n",
    "  | [Step with error] | [Error Type] | [Error Type] | [Correct/Incorrect] |\n",
    "\n",
    "  **Summary Statistics:**\n",
    "  - Total correctly classified errors: [X]/[Total Errors]\n",
    "  - Error classification accuracy: [X]%\n",
    "\n",
    "  ## Part 3: Documentation Quality\n",
    "  | Criterion | Rating | Explanation |\n",
    "  |-----------|--------|-------------|\n",
    "  | Structure | [Excellent/Good/Poor] | [Explanation] |\n",
    "  | Tense | [Excellent/Good/Poor] | [Explanation] |\n",
    "  | Language | [Excellent/Good/Poor] | [Explanation] |\n",
    "  | Numbering | [Excellent/Good/Poor] | [Explanation] |\n",
    "  | Timing | [Excellent/Good/Poor] | [Explanation] |\n",
    "\n",
    "  ## Overall Assessment\n",
    "  [Provide a concise overall assessment of the AI documentation's quality, highlighting key strengths and weaknesses, with suggestions for improvement.]\n",
    "\n",
    "  # Input Materials\n",
    "  ## Benchmark Documentation (Ground Truth)\n",
    "  \n",
    "  \"\"\"\n",
    "]\n",
    "inputs.extend(documentation_input2)\n",
    "inputs.extend([\"\"\"\n",
    "  ## AI-Generated Documentation\n",
    "\"\"\"])\n",
    "inputs.extend(documentation)\n",
    "\n",
    "model = GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    inputs, \n",
    "    generation_config=GenerationConfig(\n",
    "        temperature=0.9,\n",
    "        # audio_timestamp=True # Supported if only one video or audio is used\n",
    "    )\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model did not return valid JSON.\n",
      "Model output:\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"Step\": \"Prerequisite 1\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"Prerequisite 2\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"1\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"2\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"3\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"False Positive\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"4\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"False Positive\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"5\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"False Positive\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"6\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"7\",\n",
      "    \"Benchmark\": \"Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"False Negative\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"8\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"False Positive\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"9\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"10\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"11\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"12\",\n",
      "    \"Benchmark\": \"Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"Error (Correctly Identified)\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"13\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"14\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"Error\",\n",
      "    \"Classification\": \"False Positive\"\n",
      "  },\n",
      "  {\n",
      "    \"Step\": \"15\",\n",
      "    \"Benchmark\": \"No Error\",\n",
      "    \"AI Response\": \"No Error\",\n",
      "    \"Classification\": \"No Error\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your configured Vertex AI GenerativeModel instance\n",
    "\n",
    "def get_table_json_prompt(text_with_tables: str, table_identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a prompt to extract a specific table from text into JSON.\n",
    "\n",
    "    Args:\n",
    "        text_with_tables: The full text containing the table(s).\n",
    "        table_identifier: A string to help the model identify the target table\n",
    "                          (e.g., the table title, or a unique phrase near it).\n",
    "\n",
    "    Returns:\n",
    "        A formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data extraction tool.\n",
    "    Your task is to locate a specific table within the provided text and output its data as a JSON array.\n",
    "\n",
    "    Here is the text containing the table(s):\n",
    "    ---TEXT_START---\n",
    "    {text_with_tables}\n",
    "    ---TEXT_END---\n",
    "\n",
    "    Identify the table that best matches the following description or title: \"{table_identifier}\"\n",
    "\n",
    "    Output the data from ONLY this table as a valid JSON array. Each object in the array should represent a row from the table. The keys of each object should be the exact column headers from the identified table.\n",
    "\n",
    "    Output Constraints:\n",
    "    - Do not include any introductory or explanatory text (e.g., \"Here is the JSON:\").\n",
    "    - Do not include any text before or after the JSON object.\n",
    "    - The output must be *only* the valid JSON array structure itself.\n",
    "    - If the specified table cannot be found, output an empty JSON array: []\n",
    "\n",
    "    Answer direct with the JSON.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "text_from_previous_response = response.text\n",
    "table_to_extract = \"Part 1: Error Identification Accuracy\"\n",
    "# table_to_extract = \"Part 2: Error Classification Accuracy\"\n",
    "\n",
    "prompt_for_extraction = get_table_json_prompt(text_from_previous_response, table_to_extract)\n",
    "json_response = model.generate_content([prompt_for_extraction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted and parsed JSON:\n",
      "\n",
      "DataFrame created:\n",
      "              Step Benchmark AI Response                Classification\n",
      "0   Prerequisite 1  No Error    No Error                      No Error\n",
      "1   Prerequisite 2  No Error    No Error                      No Error\n",
      "2                1  No Error    No Error                      No Error\n",
      "3                2  No Error    No Error                      No Error\n",
      "4                3  No Error       Error                False Positive\n",
      "5                4  No Error       Error                False Positive\n",
      "6                5  No Error       Error                False Positive\n",
      "7                6  No Error    No Error                      No Error\n",
      "8                7     Error    No Error                False Negative\n",
      "9                8  No Error       Error                False Positive\n",
      "10               9  No Error    No Error                      No Error\n",
      "11              10  No Error    No Error                      No Error\n",
      "12              11  No Error    No Error                      No Error\n",
      "13              12     Error       Error  Error (Correctly Identified)\n",
      "14              13  No Error    No Error                      No Error\n",
      "15              14  No Error       Error                False Positive\n",
      "16              15  No Error    No Error                      No Error\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# This is the full string you received from the model\n",
    "model_output_string = json_response.text\n",
    "\n",
    "# Define the markers for the JSON code block\n",
    "start_marker = \"```json\"\n",
    "end_marker = \"```\"\n",
    "\n",
    "# Find the position of the markers\n",
    "start_index = model_output_string.find(start_marker)\n",
    "end_index = model_output_string.find(end_marker, start_index + len(start_marker)) # Search for end marker after the start\n",
    "\n",
    "json_data = None\n",
    "extracted_json_string = \"\"\n",
    "\n",
    "if start_index != -1 and end_index != -1:\n",
    "    # Extract the string between the markers\n",
    "    # Add the length of the start_marker to get the content *after* it\n",
    "    extracted_json_string = model_output_string[start_index + len(start_marker) : end_index].strip()\n",
    "\n",
    "    # Now, try to parse the extracted string as JSON\n",
    "    try:\n",
    "        json_data = json.loads(extracted_json_string)\n",
    "        print(\"Successfully extracted and parsed JSON:\")\n",
    "        # print(json.dumps(json_data, indent=2)) # Optional: print for verification\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON after extraction: {e}\")\n",
    "        print(\"Extracted string:\")\n",
    "        print(extracted_json_string)\n",
    "else:\n",
    "    print(\"Could not find JSON code block markers in the output.\")\n",
    "    print(\"Model output:\")\n",
    "    print(model_output_string)\n",
    "\n",
    "# Now 'json_data' holds your data as a Python list of dictionaries\n",
    "# You can use it directly or convert it to a pandas DataFrame:\n",
    "if json_data is not None:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(json_data)\n",
    "    print(\"\\nDataFrame created:\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Benchmark</th>\n",
       "      <th>AI Response</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prerequisite 1</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prerequisite 2</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>No Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>False Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>No Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>False Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>No Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>False Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>False Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>No Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>False Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error (Correctly Identified)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>No Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>False Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "      <td>No Error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Step Benchmark AI Response                Classification\n",
       "0   Prerequisite 1  No Error    No Error                      No Error\n",
       "1   Prerequisite 2  No Error    No Error                      No Error\n",
       "2                1  No Error    No Error                      No Error\n",
       "3                2  No Error    No Error                      No Error\n",
       "4                3  No Error       Error                False Positive\n",
       "5                4  No Error       Error                False Positive\n",
       "6                5  No Error       Error                False Positive\n",
       "7                6  No Error    No Error                      No Error\n",
       "8                7     Error    No Error                False Negative\n",
       "9                8  No Error       Error                False Positive\n",
       "10               9  No Error    No Error                      No Error\n",
       "11              10  No Error    No Error                      No Error\n",
       "12              11  No Error    No Error                      No Error\n",
       "13              12     Error       Error  Error (Correctly Identified)\n",
       "14              13  No Error    No Error                      No Error\n",
       "15              14  No Error       Error                False Positive\n",
       "16              15  No Error    No Error                      No Error"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Part 1: Error Identification Accuracy\\n\\n| Step | Benchmark | AI Response | Classification |\\n|------|-----------|-------------|----------------|\\n| Prerequisite 1 | No Error | No Error | No Error |\\n| Prerequisite 2 | No Error | No Error | No Error |\\n| 1 | No Error | No Error | No Error |\\n| 2 | No Error | No Error | No Error |\\n| 3 | No Error | Error | False Positive |\\n| 4 | No Error | Error | False Positive |\\n| 5 | No Error | Error | False Positive |\\n| 6 | No Error | No Error | No Error |\\n| 7 | Error | No Error | False Negative |\\n| 8 | No Error | Error | False Positive |\\n| 9 | No Error | No Error | No Error |\\n| 10 | No Error | No Error | No Error |\\n| 11 | No Error | No Error | No Error |\\n| 12 | Error | Error | Correctly Identified |\\n| 13 | No Error | No Error | No Error |\\n| 14 | No Error | Error | False Positive |\\n| 15 | No Error | No Error | No Error |\\n\\n**Summary Statistics:**\\n- Total correct identifications: 10/17\\n- Total false positives: 6\\n- Total false negatives: 1\\n- Overall accuracy: 58.8%\\n\\n## Part 2: Error Classification Accuracy\\n\\n| Step | Benchmark Error Type | AI Error Type | Classification |\\n|------|---------------------|---------------|----------------|\\n| 12 | Omitted | Omitted | Correct |\\n\\n**Summary Statistics:**\\n- Total correctly classified errors: 1/1\\n- Error classification accuracy: 100%\\n\\n## Part 3: Documentation Quality\\n\\n| Criterion | Rating | Explanation |\\n|-----------|--------|-------------|\\n| Structure | Poor | It added \"Expected Results\" and \"Figures\" which should not be there |\\n| Tense | Good | Generally uses past tense, but includes instructional language in some of the steps |\\n| Language | Poor | Includes instructional language, checkmarks, and justifications.  Also not all observations |\\n| Numbering | Excellent | Maintained step numbering of the original protocol |\\n| Timing | Excellent | Includes exact actual timing |\\n\\n## Overall Assessment\\nThe AI documentation\\'s quality is poor due to the language style used, which includes instructional text and justifications instead of being purely observational. Also it included unnecessary sections. The tense is mostly correct. The numbering is accurate. There are several false positives and one false negative.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## Documentation:',\n",
       " file_data {\n",
       "   mime_type: \"text/md\"\n",
       "   file_uri: \"gs://mannlab_videos/compare_protocol_video/QueueSamples_docuWrongRow_S3A1Twice.md\"\n",
       " }]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentation_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Instruction\n",
       "You are an expert evaluator specializing in scientific protocol documentation. Your task is to evaluate the quality of an AI-generated documentation against a benchmark documentation (ground truth). You will be provided with an AI-generated documentation and a benchmark documentation (human-verified ground truth).\n",
       "\n",
       "Your evaluation will have three distinct parts, each focused on different aspects of the AI's performance.\n",
       "\n",
       "# Evaluation Parts\n",
       "## Part 1: Error Identification Accuracy\n",
       "For each step in the protocol, determine if the AI correctly identified the presence or absence of errors by classifying into one of these categories:\n",
       "- **No Error**: Both benchmark and AI response agree there was no error\n",
       "- **Error (Correctly Identified)**: Both benchmark and AI response agree there was an error\n",
       "- **False Positive**: AI response claimed an error when the benchmark indicates none\n",
       "- **False Negative**: AI response missed an error that the benchmark shows\n",
       "\n",
       "## Part 2: Error Type Classification\n",
       "For each error that was correctly identified by both the benchmark and AI response, determine if the AI correctly classified the error type:\n",
       "- **Correct Classification**: AI used the same error type as the benchmark (Omitted, Error, Deviation, Added)\n",
       "- **Incorrect Classification**: AI used a different error type than the benchmark\n",
       "\n",
       "## Part 3: Documentation Quality\n",
       "Evaluate the AI's documentation quality based on these criteria:\n",
       "1. **Structure**: Did it keep only relevant sections: Aim, Materials, Procedure, Results?\n",
       "2. **Tense**: Did it use past tense to describe what actually happened, not what should happen?\n",
       "3. **Language**: Did it remove all instructional language and replace with observations?\n",
       "4. **Numbering**: Did it maintain step numbering of the original protocol even if order changed?\n",
       "5. **Timing**: Did it include exact actual timing, not estimated timing?\n",
       "\n",
       "# Rating Rubric\n",
       "For each part, provide an evaluation:\n",
       "\n",
       "### Part 1: Error Identification Accuracy\n",
       "- Calculate and report:\n",
       "  - Total number of correct identifications (No Error + Correctly Identified Error)\n",
       "  - Total number of false positives\n",
       "  - Total number of false negatives\n",
       "  - Overall accuracy percentage (correct identifications / total steps)\n",
       "\n",
       "### Part 2: Error Type Classification\n",
       "- Calculate and report:\n",
       "  - Total errors correctly classified / Total errors correctly identified\n",
       "  - Overall error classification accuracy percentage\n",
       "\n",
       "### Part 3: Documentation Quality\n",
       "For each criterion:\n",
       "- **Excellent**: The criterion was fully met with no issues\n",
       "- **Good**: The criterion was mostly met with minor issues\n",
       "- **Poor**: The criterion was not met or had significant issues\n",
       "\n",
       "# Evaluation Steps\n",
       "1. Create a table for each step in the protocol showing error identification accuracy\n",
       "2. Analyze correctly identified errors to determine classification accuracy\n",
       "3. Evaluate documentation quality against the 5 criteria\n",
       "4. Provide final scores and overall assessment\n",
       "5. Highlight specific strengths and areas for improvement\n",
       "\n",
       "# Input Materials\n",
       "## AI-Generated Documentation\n",
       "Okay, Professor Mann, here's the corrected documentation based on the provided video, reflecting the actual actions performed and their timing.\n",
       "\n",
       "## Documentation:# Queue and measure samples in HyStar\n",
       "\n",
       "## Aim\n",
       "Queueing samples in HyStar for LC-MS measurement.\n",
       "\n",
       "## Materials\n",
       "\n",
       "### Software\n",
       "HyStar 6.0\n",
       "\n",
       "## Procedure\n",
       "Timing: 3 minutes\n",
       "\n",
       "Prerequisite 1. ✓ Mentioned that 5 ng HeLa Evotips were placed at S1 from A1 to A6 and blanks at S3 from A1 to A6.\n",
       "\n",
       "Prerequisite 2. ✓ Reported that the TIMS device had already been calibrated.\n",
       "\n",
       "1. ✓ Navigated to the 'Acquisition' tab in HyStar.\n",
       "2. ✓ Selected an already existing sample table by pressing the arrow down button when hovering over the sample table name in the left sample table column.\n",
       "\n",
       "3. ❌ **Omitted:** Copied already existing sample table entries to modify them\n",
       "4. ⚠️ **Deviation:** Manually Adjusted the sample ID without following this pattern: currentDate_massSpec_user_sampleType_projectID_ sampleName, instead created \"THMS50tcep_PAlk_SA_blank\".\n",
       "\n",
       "5. ⚠️ **Deviation:** The queue did not contain three dda-PASEF or three dia-PASEF runs. The queue consisted of multiple rows with samples labelled \"THMS50tcep_PAlk_SA_blank\" and \"THMS50tcep_PAlk_MA_HeLa\"\n",
       "    \n",
       "6. ✓ Verified the column autocompletion settings with right-click on a field in the column 'vial'. The arrows pointet from A1-A12, indicating that values increased to the right. The tray type was set to 'Evosep' and slots 1-6 were designated as '96Evotip'.\n",
       "\n",
       "7. ✓ Matched the Evotip position with the sample's location in the Evotip box. The first Evotip was placed in position S1 A1, and all remaining positions were specified individually and automatically by dragging the values.\n",
       "8. ⚠️ **Deviation:** Path was not explicity specified.\n",
       "9. ✓ The separation method \"WhisperRj.zoom\" was selected.\n",
       "\n",
       "10. ✓ Injection method was set to 'standard'.\n",
       "\n",
       "11. ✓ The MS method \"20240703_DDA_maintenance_ionOptics_100ms_m/z713_300-1200_HS_1800V\" was selected.\n",
       "\n",
       "12. ❌ **Omitted:** Idle flow on the Evosep was not canceled.\n",
       "13. ✓ Saved the sample table.\n",
       "\n",
       "14. ⚠️ **Deviation:** Only the last row was selected to upload sample conditions, instead of all rows. The status changed to loaded.\n",
       "\n",
       "15. ✓ Pressed 'start' and the sequence started to run.\n",
       "\n",
       "## Expected Results\n",
       "- ✓ The sample table was running.\n",
       "\n",
       "## Figures\n",
       "\n",
       "### Figure 1: Hystar\n",
       "- Screenshot of Hystar settings\n",
       "\n",
       "\n",
       "## Benchmark Documentation (Ground Truth)\n",
       "['## Documentation:', file_data {\n",
       "  mime_type: \"text/md\"\n",
       "  file_uri: \"gs://mannlab_videos/compare_protocol_video/QueueSamples_docuWrongRow_S3A1Twice.md\"\n",
       "}\n",
       "]\n",
       "\n",
       "# Output Format\n",
       "## Part 1: Error Identification Accuracy\n",
       "| Step | Benchmark | AI Response | Classification |\n",
       "|------|-----------|-------------|----------------|\n",
       "| [Step details] | [Error/No Error] | [Error/No Error] | [No Error/Error/False Positive/False Negative] |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correct identifications: [X]/[Total Steps]\n",
       "- Total false positives: [X]\n",
       "- Total false negatives: [X]\n",
       "- Overall accuracy: [X]%\n",
       "\n",
       "## Part 2: Error Classification Accuracy\n",
       "| Step | Benchmark Error Type | AI Error Type | Classification |\n",
       "|------|---------------------|---------------|----------------|\n",
       "| [Step with error] | [Error Type] | [Error Type] | [Correct/Incorrect] |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correctly classified errors: [X]/[Total Errors]\n",
       "- Error classification accuracy: [X]%\n",
       "\n",
       "## Part 3: Documentation Quality\n",
       "| Criterion | Rating | Explanation |\n",
       "|-----------|--------|-------------|\n",
       "| Structure | [Excellent/Good/Poor] | [Explanation] |\n",
       "| Tense | [Excellent/Good/Poor] | [Explanation] |\n",
       "| Language | [Excellent/Good/Poor] | [Explanation] |\n",
       "| Numbering | [Excellent/Good/Poor] | [Explanation] |\n",
       "| Timing | [Excellent/Good/Poor] | [Explanation] |\n",
       "\n",
       "## Overall Assessment\n",
       "[Provide a concise overall assessment of the AI documentation's quality, highlighting key strengths and weaknesses, with suggestions for improvement.]\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
