{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation assistant\n",
    "\n",
    "This notebook demonstrates a documentation assistant: Video-to-documentation conversion using Vertex AI\n",
    "\n",
    "Converting videos-to-documentation involves three steps: \n",
    "1. Protocol finder: Select protocol which best captures the step being performed in the video\n",
    "2. Video comparing to ground-of-truth protocol → lab documentation + errors in procedure\n",
    "3. Analytics based on benchmark dataset: automatic comparison of errors found by documentation assistent vs actual errors\n",
    "\n",
    "In this notebook, I will focus on the step two and three - Compare video with protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../secrets.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_to_append = Path(Path.cwd()).parent / \"proteomics_specialist\"\n",
    "sys.path.append(str(path_to_append))\n",
    "import video_to_protocol\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")\n",
    "\n",
    "PROJECT_ID = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "vertexai.init(project=PROJECT_ID, location=\"europe-west9\")  # europe-west9 is Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "\n",
    "# Initialize Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"mannlab_videos\"\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "from typing import TYPE_CHECKING, NamedTuple\n",
    "\n",
    "def generate_content_from_model(\n",
    "    inputs: Any,\n",
    "    model_name: str = \"gemini-2.0-flash\",\n",
    "    temperature: float = 0.9,\n",
    ") -> tuple:\n",
    "    \"\"\"Generate content using Google's Generative AI model.\n",
    "    \n",
    "    This function sends inputs to a specified Gemini model and returns the \n",
    "    generated response along with usage metadata.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : Any\n",
    "        The inputs to send to the model (text, images, or videos).\n",
    "    model_name : str, default=\"gemini-2.0-flash\"\n",
    "        Name of the generative model to use.\n",
    "    temperature : float, default=0.9\n",
    "        Controls the randomness of the output. Higher values (closer to 1.0)\n",
    "        make output more random, lower values make it more deterministic.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing (response_text, usage_metadata)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the model fails to generate content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = GenerativeModel(model_name)\n",
    "        \n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            # Uncomment if using single audio/video input\n",
    "            # audio_timestamp=True\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(\n",
    "            inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        documentation = response.text\n",
    "        usage_metadata = response.usage_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error during content generation\")\n",
    "        raise ValueError(f\"Failed to generate content: {str(e)}\")\n",
    "    \n",
    "    return documentation, usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "\n",
    "def prepare_all_inputs(\n",
    "    lab_video_path: str,\n",
    "    protocol_path: str,\n",
    "    documentation_video_path: str,\n",
    "    documentation_path: str,\n",
    "    bucket: str,\n",
    "    prefix: str = \"compare_protocol_video\"\n",
    ") -> dict:\n",
    "    \"\"\"Prepare all four standard inputs for the generative model.\n",
    "    \n",
    "    This function uploads the four standard files (lab video, protocol document, \n",
    "    documentation video, and documentation document) and formats them as inputs \n",
    "    for a generative model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lab_video_path : str\n",
    "        Path to the lab video file.\n",
    "    protocol_path : str\n",
    "        Path to the protocol markdown file.\n",
    "    documentation_video_path : str\n",
    "        Path to the documentation video file.\n",
    "    documentation_path : str\n",
    "        Path to the documentation markdown file.\n",
    "    bucket : str\n",
    "        GCS bucket name for uploading the files.\n",
    "    prefix : str, default=\"compare_protocol_video\"\n",
    "        Prefix for the files in GCS bucket.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the four formatted inputs:\n",
    "        'protocol_video_input', 'protocol_input', 'lab_video_input', 'documentation_input'\n",
    "    \"\"\"\n",
    "    \n",
    "    video_uri = video_to_protocol.upload_video_to_gcs(lab_video_path, bucket, prefix)\n",
    "    file_extension = os.path.splitext(video_uri)[1].lower()[1:]\n",
    "    protocol_video_input = [Part.from_uri(video_uri, mime_type=f\"video/{file_extension}\")]\n",
    "    \n",
    "    uri = video_to_protocol.upload_video_to_gcs(protocol_path, bucket, prefix)\n",
    "    protocol_input = [Part.from_uri(uri, mime_type=\"text/md\")]\n",
    "    \n",
    "    video_uri = video_to_protocol.upload_video_to_gcs(documentation_video_path, bucket, prefix)\n",
    "    lab_video_input = [Part.from_uri(video_uri, mime_type=\"video/mp4\")]\n",
    "\n",
    "    uri = video_to_protocol.upload_video_to_gcs(documentation_path, bucket, prefix)\n",
    "    documentation_input = [Part.from_uri(uri, mime_type=\"text/md\")]\n",
    "    \n",
    "    return {\n",
    "        'protocol_video_input': protocol_video_input,\n",
    "        'protocol_input': protocol_input,\n",
    "        'lab_video_input': lab_video_input,\n",
    "        'documentation_input': documentation_input\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_benchmark_dataset(csv_path, protocol_videos_base, documentation_videos_base, markdown_base, bucket, prefix):\n",
    "    \"\"\"\n",
    "    Process the first two rows in the benchmark dataset CSV and prepare model inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing benchmark dataset information\n",
    "    protocol_videos_base : str\n",
    "        Base path to the protocol videos directory\n",
    "    documentation_videos_base : str\n",
    "        Base path to the documentation videos directory\n",
    "    markdown_base : str\n",
    "        Base path to the markdown files directory\n",
    "    bucket : object\n",
    "        The bucket object used in the prepare_all_inputs function\n",
    "    prefix : str\n",
    "        Prefix for the files in GCS bucket.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing all model inputs for the first two rows in the CSV,\n",
    "        with experiment names as keys\n",
    "    \"\"\"\n",
    "    \n",
    "    benchmark_df = pd.read_csv(\n",
    "        csv_path, \n",
    "        sep=';'\n",
    "    )\n",
    "    \n",
    "    all_model_inputs = {}\n",
    "    \n",
    "    for index, row in benchmark_df.head(2).iterrows(): # for testing .head(2).iterrows() or .iloc[[13, 14]] .iloc[::2]\n",
    "        lab_video_path = os.path.join(protocol_videos_base, row[\"protocol video\"])\n",
    "        protocol_path = os.path.join(markdown_base, row[\"protocol\"])\n",
    "        documentation_video_path = os.path.join(documentation_videos_base, row[\"documentation video\"])\n",
    "        documentation_path = os.path.join(markdown_base, row[\"documentation\"])\n",
    "        \n",
    "        dict_model_inputs = prepare_all_inputs(\n",
    "            lab_video_path,\n",
    "            protocol_path,\n",
    "            documentation_video_path,\n",
    "            documentation_path,\n",
    "            bucket,\n",
    "            prefix\n",
    "        )\n",
    "        dict_model_inputs['error_dict'] = row[\"error_dict\"]\n",
    "        \n",
    "        experiment_name = row[\"documentation\"].split(\".\")[0]\n",
    "        all_model_inputs[experiment_name] = dict_model_inputs\n",
    "        \n",
    "        print(f\"Processed {experiment_name}\")\n",
    "        \n",
    "    return all_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation test 1\n",
    "\n",
    "def generate_documentation_evaluation(benchmark_example, documentation_input, documentation, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Generate an evaluation of AI-generated documentation against benchmark documentation.\n",
    "    Build on 'ESIsourceToUltraSource_docuFogotOvenPowerSupply'.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documentation_input : list\n",
    "        The benchmark documentation (ground truth) represented as a list of strings\n",
    "    documentation : list\n",
    "        The AI-generated documentation to evaluate represented as a list of strings\n",
    "    model_name : str, optional\n",
    "        The model to use for evaluation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing (evaluation_text, usage_metadata)\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        \"\"\"\n",
    "        # Instruction\n",
    "        You are an expert evaluator specializing in scientific protocol documentation. Your task is to evaluate the error identification accuracy, error type classification and documentation quality of an AI-generated documentation against a benchmark documentation (ground truth). You will be provided with an AI-generated documentation and a benchmark documentation (human-verified ground truth). \n",
    "\n",
    "        # Evaluation Parts\n",
    "        ## Part 1: Error Identification Accuracy\n",
    "        For each step in the protocol, determine if the AI correctly identified the presence or absence of errors by classifying into one of these categories:\n",
    "        - **No Error**: Both benchmark and AI response agree there was no error\n",
    "        - **Error (Correctly Identified)**: Both benchmark and AI response agree there was an error\n",
    "        - **False Positive**: AI response claimed an error when the benchmark indicates none\n",
    "        - **False Negative**: AI response missed an error that the benchmark shows\n",
    "\n",
    "        ## Part 2: Error Type Classification\n",
    "        For each error that was correctly identified by both the benchmark and AI response, determine if the AI correctly classified the error type:\n",
    "        - **Correct Classification**: AI used the same error type as the benchmark (Omitted, Error, Deviation, Added)\n",
    "        - **Incorrect Classification**: AI used a different error type than the benchmark\n",
    "\n",
    "        ## Part 3: Documentation Quality\n",
    "        Evaluate the AI's documentation quality based on these criteria:\n",
    "        1. **Structure**: Did it keep only relevant sections: Aim, Materials, Procedure, Results?\n",
    "        2. **Tense**: Did it use past tense to describe what actually happened, not what should happen?\n",
    "        3. **Language**: Did it remove all instructional language and replace with observations?\n",
    "        4. **Numbering**: Did it maintain step numbering of the original protocol even if order changed?\n",
    "        5. **Timing**: Did it include exact actual timing, not estimated timing?\n",
    "\n",
    "        ### Rating Rubric for Part 3: Documentation Quality\n",
    "        For each criterion:\n",
    "        - **Excellent**: The criterion was fully met with no issues\n",
    "        - **Good**: The criterion was mostly met with minor issues\n",
    "        - **Poor**: The criterion was not met or had significant issues\n",
    "\n",
    "        # Evaluation Steps\n",
    "        1. Create a table for each step in the protocol showing error identification accuracy\n",
    "        2. Analyze correctly identified errors to determine classification accuracy\n",
    "        3. Evaluate documentation quality against the 5 criteria\n",
    "\n",
    "        It is very important to you to to be very exact. therefore, you always correctly reflect the errors from the benchmark documentation and identify the errors made in the AI-generated documentation. You also always provide the correct output format.\n",
    "\n",
    "        # Output Format\n",
    "        ## Part 1: Error Identification Accuracy\n",
    "        | Step | Benchmark | AI Response | Classification |\n",
    "        |------|-----------|-------------|----------------|\n",
    "        | [Step details] | [Error/No Error] | [Error/No Error] | [No Error/Error/False Positive/False Negative] |\n",
    "\n",
    "        ## Part 2: Error Classification Accuracy\n",
    "        | Step | Benchmark Error Type | AI Error Type | Classification |\n",
    "        |------|---------------------|---------------|----------------|\n",
    "        | [Step with error] | [Error Type] | [Error Type] | [Correct/Incorrect] |\n",
    "\n",
    "        ## Part 3: Documentation Quality\n",
    "        | Criterion | Rating | Explanation |\n",
    "        |-----------|--------|-------------|\n",
    "        | Structure | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Tense | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Language | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Numbering | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Timing | [Excellent/Good/Poor] | [Explanation] |\n",
    "        \n",
    "        \"\"\"\n",
    "    ]\n",
    "    inputs.extend([\"\"\"\n",
    "        # Example\n",
    "        ## Benchmark Documentation (Ground Truth)\n",
    "    \"\"\"])\n",
    "    inputs.extend(benchmark_example)\n",
    "    inputs.extend([\"## AI-Generated Documentation\"])\n",
    "    documentation_example = \"Alright, here is the documentation following your specifications:\\n\\n## Documentation:# Change source: ESI source to UltraSource\\n\\n## Abstract\\nThis protocol describes the procedure for switching from the ESI source to UltraSource.\\n\\n## Materials\\n\\n### Equipment\\n- timsTOF Ultra Mass Spectrometer:\\n  - ESI ion source\\n  - UltraSource ion source \\n- IonOpticks Column\\n- Evosep One LC System with sample line\\n- NanoViper Adapter (black)\\n- Pliers\\n\\n## Procedure\\n\\n*Estimated timing: less than 10 minute*\\n\\n### Switch timsTOF to standby\\n\\n1. ✓ Verified the instrument was on standby mode\\n2. ✓ Verified the syringe was inactive\\n3. ✓ Selected 'CaptiveSpray' but did not activate it yet\\n\\n### Remove ESI source\\n\\n4. ✓ Disconnected the peak connector of the sample tubing\\n5. ✓ Disconnected the nebulizer N₂ line\\n6. ✓ Removed the source door. Hinged it out\\n7. ❌ **Omitted:** Put on gloves after removing source door\\n8. ✓ Removed the spray shield, and capillary cap.\\n9. ⚠️ **Deviation:** Inspected the capillary position and gently pushed it back into proper position \\n\\n### Mount UltraSource\\n\\n10. ✓ Hinged the UltraSource door in and closed it \\n11. ✓ Slid the UltraSource housing onto the source door and secured it by flipping the handles\\n12. ✓ Connected the filter tubing to the source\\n\\n### Connect column and sample line\\n\\n13. ✓ Noted an IonOpticks column already inside UltraSource \\n14. ✓ Noted the LC sample line had NanoViper adapter already attached\\n15. ❌ **Omitted:** No need to snipp access liquid\\n16. ✓ Held the column fititng of the IonOpticks column with a pliers.\\n17. ✓ Hand-tightened the NanoViper of the LC sample line with the column fitting \\n18. ✓ Drew the oven closer to the UltraSource, and secured it \\n19. ✓ Removed the NanoViper adapter \\n20. ✓ Placed the metal grounding screw\\n21. ✓ Closed the lid of the oven\\n22. ✓ Connected the oven to the electrical power supply\\n23. ✓ Noted that with the correct temperature\\n\\n### Switch timsTOF to operate and idle flow\\n\\n24. ✓ Noted the CaptiveSpray function in timsControl had been activated.\\n25. ✓ Noted that the instrument was on the operational mode\\n26. ✓ Noted the idle flow was active\\n27. ✓ Stay in timsControl\\n28. ⚠️ **Deviation:** Checked the MS signal. Noted it needed to be adjusted to between 9-11 mbar\\n\\n## Expected Results\\n- In timsControl, signal intensity should be above 10^7\\n- Stable signal in in timsControl\\n\\n\"\n",
    "    inputs.extend([documentation_example])\n",
    "    inputs.extend([\"## Evaluation\"])\n",
    "    evaluation_example = '## Part 1: Error Identification Accuracy\\n| Step | Benchmark | AI Response | Classification |\\n|------|-----------|-------------|----------------|\\n| 1 | No Error | No Error | No Error |\\n| 2 | No Error | No Error | No Error |\\n| 3 | No Error | No Error | No Error |\\n| 4 | No Error | No Error | No Error |\\n| 5 | No Error | No Error | No Error |\\n| 6 | No Error | No Error | No Error |\\n| 7 | Error | Error | Error (Correctly Identified) |\\n| 8 | No Error | No Error | No Error |\\n| 9 | Error | Error | Error (Correctly Identified) |\\n| 10 | No Error | No Error | No Error |\\n| 11 | No Error | No Error | No Error |\\n| 12 | No Error | No Error | No Error |\\n| 13 | No Error | No Error | No Error |\\n| 14 | No Error | No Error | No Error |\\n| 15 | No Error | Error | False Positive |\\n| 16 | No Error | No Error | No Error |\\n| 17 | No Error | No Error | No Error |\\n| 18 | No Error | No Error | No Error |\\n| 19 | Error | No Error | False Negative |\\n| 20 | No Error | No Error | No Error |\\n| 21 | No Error | No Error | No Error |\\n| 22 | Error | No Error | False Negative |\\n| 23 | Error | No Error | False Negative |\\n| 24 | No Error | No Error | No Error |\\n| 25 | No Error | No Error | No Error |\\n| 26 | Error | No Error | False Negative |\\n| 27 | Error | No Error | False Negative |\\n| 28 | No Error | Error | False Positive |\\n| 29 | Error | No Error | False Negative |\\n## Part 2: Error Classification Accuracy\\n| Step | Benchmark Error Type | AI Error Type | Classification |\\n|------|---------------------|---------------|----------------|\\n| 7 | Omitted | Omitted | Correct |\\n| 9 | Omitted | Deviation | Incorrect |\\\\n\\n## Part 3: Documentation Quality\\n| Criterion | Rating | Explanation |\\n|-----------|--------|-------------|\\n| Structure | Good | The structure is generally good, maintaining the Aim, Materials, Procedure, and Results sections. However, the Abstract should have been rephrased to aim and the actuall timing and results should have been stated. |\\n| Tense | Good | The tense is mostly past tense, but there are instances of present tense slipping in (\"Stay in timsControl\"). |\\n| Language | Good | The language is mostly observational, but some instructional language remains. |\\n| Numbering | Poor | Step 19 occured between step 17 & step 18 and step 29 occured between step 24 & step 25. Both were not placed correctly. |\\n| Timing | Poor | The AI-generated documentation provided an Estimated Timing which is incorrect. |\\n'\n",
    "\n",
    "    inputs.extend([evaluation_example])\n",
    "\n",
    "    inputs.extend([\"\"\"\n",
    "        # Input Materials\n",
    "        ## Benchmark Documentation (Ground Truth)\n",
    "    \"\"\"])\n",
    "    inputs.extend(documentation_input)\n",
    "    \n",
    "    inputs.extend([\"## AI-Generated Documentation\"])\n",
    "    inputs.extend([documentation])\n",
    "    inputs.extend([\"## Evaluation\"])\n",
    "\n",
    "    evaluation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return evaluation, usage_metadata\n",
    "\n",
    "def get_table_json_prompt(text_with_tables: str, table_identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a prompt to extract a specific table from text into JSON.\n",
    "\n",
    "    Args:\n",
    "        text_with_tables: The full text containing the table(s).\n",
    "        table_identifier: A string to help the model identify the target table\n",
    "                          (e.g., the table title, or a unique phrase near it).\n",
    "\n",
    "    Returns:\n",
    "        A formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data extraction tool.\n",
    "    Your task is to locate a specific table within the provided text and output its data as a JSON array.\n",
    "\n",
    "    Here is the text containing the table(s):\n",
    "    ---TEXT_START---\n",
    "    {text_with_tables}\n",
    "    ---TEXT_END---\n",
    "\n",
    "    Identify the table that best matches the following title: \"{table_identifier}\"\n",
    "\n",
    "    It is very important to you to output the data from ONLY this table as a valid JSON array. Each object in the array should represent a row from the table. The keys of each object should be the exact column headers from the identified table.\n",
    "\n",
    "    Output Constraints:\n",
    "    - Answer direct with the JSON.\n",
    "    - If the specified table cannot be found, output an empty JSON array: []\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_json_from_model_output(model_output_string):\n",
    "    \"\"\"\n",
    "    Extract and parse JSON data from a model output string that contains JSON within code block markers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_output_string : str\n",
    "        The string output from the model that contains JSON within code block markers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe: A pandas DataFrame created from the JSON data, or None if extraction failed\n",
    "    \"\"\"\n",
    "    start_marker = \"```json\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    start_index = model_output_string.find(start_marker)\n",
    "    end_index = model_output_string.find(end_marker, start_index + len(start_marker))  # Search for end marker after the start\n",
    "    \n",
    "    df = None\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        extracted_json_string = model_output_string[start_index + len(start_marker):end_index].strip()\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(extracted_json_string)\n",
    "            logger.info(\"Successfully extracted and parsed JSON.\")\n",
    "            \n",
    "            if isinstance(json_data, list) and all(isinstance(item, dict) for item in json_data):\n",
    "                df = pd.DataFrame(json_data)\n",
    "            else:\n",
    "                logger.warning(\"JSON data is not a list of dictionaries, could not create DataFrame.\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON after extraction: {e}\")\n",
    "            logger.debug(f\"Extracted string: {extracted_json_string}\")\n",
    "    else:\n",
    "        logger.error(\"Could not find JSON code block markers in the output.\")\n",
    "        logger.debug(f\"Model output: {model_output_string}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_table_to_dataframe(evaluation, table_name, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Extract a table from evaluation content and convert it to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : str\n",
    "        The evaluation content containing tables\n",
    "    table_name : str\n",
    "        The name of the table to extract\n",
    "    model_name : str, optional\n",
    "        The model to use for content generation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the extracted table data\n",
    "    \"\"\"\n",
    "    extraction_prompt = get_table_json_prompt(evaluation, table_name)\n",
    "    \n",
    "    json_response, _ = generate_content_from_model(\n",
    "        extraction_prompt,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    results_df = extract_json_from_model_output(json_response)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_error_evaluation_metrics(evaluation):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive error evaluation metrics from an evaluation document.\n",
    "    \n",
    "    This function extracts tables from the evaluation document and calculates\n",
    "    metrics for error identification, error classification, and documentation quality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : str\n",
    "        The evaluation document containing the tables to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing all calculated metrics organized by category\n",
    "    \"\"\"\n",
    "    error_evaluation_metrics = {}\n",
    "    \n",
    "    # Part 1: Error Identification Accuracy\n",
    "    identification_table_name = \"Part 1: Error Identification Accuracy\"\n",
    "    identification_results_df = extract_table_to_dataframe(evaluation, identification_table_name)\n",
    "    \n",
    "    if identification_results_df is not None:\n",
    "        correctly_identified_rows = identification_results_df[\n",
    "            (identification_results_df[\"Classification\"] == \"No Error\") |\n",
    "            (identification_results_df[\"Classification\"] == \"Error (Correctly Identified)\")\n",
    "        ]\n",
    "        total_actual_errors = identification_results_df[identification_results_df[\"Benchmark\"] == \"Error\"]\n",
    "        correctly_identified_errors = identification_results_df[identification_results_df[\"Classification\"] == \"Error (Correctly Identified)\"]\n",
    "        false_positive_errors = identification_results_df[identification_results_df[\"Classification\"] == \"False Positive\"]\n",
    "        false_negative_errors = identification_results_df[identification_results_df[\"Classification\"] == \"False Negative\"]\n",
    "        \n",
    "        error_evaluation_metrics[\"Error Identification Statistics\"] = {\n",
    "            \"Total steps evaluated\": len(identification_results_df),\n",
    "            \"Total correct identifications\": len(correctly_identified_rows),\n",
    "            \"Overall identification accuracy\": len(correctly_identified_rows) / len(identification_results_df) if len(identification_results_df) > 0 else 0,\n",
    "            \"Error recall rate\": len(correctly_identified_errors) / len(total_actual_errors) if len(total_actual_errors) > 0 else \"N/A\",\n",
    "            \"False positive count\": len(false_positive_errors),\n",
    "            \"False negative count\": len(false_negative_errors)\n",
    "        }\n",
    "    else:\n",
    "        error_evaluation_metrics[\"Error Identification Statistics\"] = {\n",
    "            \"Status\": \"No data available\"\n",
    "        }\n",
    "    \n",
    "    # Part 2: Error Classification Accuracy\n",
    "    classification_table_name = \"Part 2: Error Classification Accuracy\"\n",
    "    classification_results_df = extract_table_to_dataframe(evaluation, classification_table_name)\n",
    "    \n",
    "    if classification_results_df is not None:\n",
    "        correctly_classified_errors = classification_results_df[classification_results_df[\"Classification\"] == \"Correct\"]\n",
    "        \n",
    "        error_evaluation_metrics[\"Error Classification Statistics\"] = {\n",
    "            \"Total errors analyzed\": len(classification_results_df),\n",
    "            \"Correctly classified errors\": len(correctly_classified_errors),\n",
    "            \"Classification accuracy\": len(correctly_classified_errors) / len(classification_results_df) if len(classification_results_df) > 0 else 0\n",
    "        }\n",
    "    else:\n",
    "        error_evaluation_metrics[\"Error Classification Statistics\"] = {\n",
    "            \"Status\": \"No data available\"\n",
    "        }\n",
    "    \n",
    "    # # Part 3: Documentation Quality\n",
    "    # documentation_table_name = \"Part 3: Documentation Quality\"\n",
    "    # documentation_quality_df = extract_table_to_dataframe(evaluation, documentation_table_name)\n",
    "\n",
    "    return error_evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation test 2\n",
    "\n",
    "def extract_errors(documentation, docu_steps, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Extract the identified errors of AI-generated documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documentation : list\n",
    "        The AI-generated documentation to extract represented as a list of strings\n",
    "    model_name : str, optional\n",
    "        The model to use for evaluation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing (evaluation_text, usage_metadata)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\\\n",
    "        # Instruction\n",
    "        You are an expert evaluator specializing in scientific protocol documentation. Your task is to extract the error positions and types of an AI-generated documentation for following steps {docu_steps}. It is very important to you to be very exact.\n",
    "\n",
    "        # Output Format\n",
    "        ## Table\n",
    "        | Step | AI Response | AI Class |\n",
    "        |------|-------------|----------------|\n",
    "        | [Step] | [Error/No Error] | [N/A, Error, Omitted, Deviation, Added] |\n",
    "        \"\"\"\n",
    "    \n",
    "    inputs = [prompt.format(docu_steps=docu_steps)  ] \n",
    "    inputs.extend([\"## AI-Generated Documentation\"])\n",
    "    inputs.extend([documentation])\n",
    "    inputs.extend([\"## Output table\"])\n",
    "\n",
    "    evaluation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return evaluation, usage_metadata\n",
    "\n",
    "def generate_documentation_evaluation(documentation_input, documentation, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Generate an evaluation of AI-generated documentation against benchmark documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documentation_input : list\n",
    "        The benchmark documentation (ground truth) represented as a list of strings\n",
    "    documentation : list\n",
    "        The AI-generated documentation to evaluate represented as a list of strings\n",
    "    model_name : str, optional\n",
    "        The model to use for evaluation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing (evaluation_text, usage_metadata)\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        \"\"\"\n",
    "        # Instruction\n",
    "        You are an expert evaluator specializing in scientific protocol documentation. Your task is to evaluate the documentation quality of an AI-generated documentation against a benchmark documentation (ground truth). \n",
    "\n",
    "        # Evaluation Parts\n",
    "\n",
    "        ## 5 Criteria:\n",
    "        Evaluate the AI's documentation quality based on these criteria:\n",
    "        1. **Structure**: Did it keep only relevant sections: Aim, Materials, Procedure, Results?\n",
    "        2. **Tense**: Did it use past tense to describe what actually happened, not what should happen?\n",
    "        3. **Language**: Did it remove all instructional language and replace with observations?\n",
    "        4. **Numbering**: Did it maintain step numbering of the original protocol even if order changed?\n",
    "        5. **Timing**: Did it include exact actual timing, not estimated timing?\n",
    "\n",
    "        ### Rating Rubric:\n",
    "        For each criterion:\n",
    "        - **Excellent**: The criterion was fully met with no issues\n",
    "        - **Good**: The criterion was mostly met with minor issues\n",
    "        - **Poor**: The criterion was not met or had significant issues\n",
    "\n",
    "        # Output Format\n",
    "        ## Documentation Quality\n",
    "        | Criterion | Rating | Explanation |\n",
    "        |-----------|--------|-------------|\n",
    "        | Structure | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Tense | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Language | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Numbering | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Timing | [Excellent/Good/Poor] | [Explanation] |\n",
    "\n",
    "        # Evaluation Steps\n",
    "        1. the documentation quality of an AI-generated documentation against a benchmark documentation (ground truth) using the  5 criteria.\n",
    "        2. Create a table summarizing the evaluation results.\n",
    "        \n",
    "        \"\"\"\n",
    "    ]\n",
    "    inputs.extend([\"\"\"\n",
    "        # Input Materials\n",
    "        ## Benchmark Documentation (Ground Truth)\n",
    "    \"\"\"])\n",
    "    inputs.extend(documentation_input)\n",
    "    \n",
    "    inputs.extend([\"## AI-Generated Documentation\"])\n",
    "    inputs.extend([documentation])\n",
    "    inputs.extend([\"# Documentation Quality\"])\n",
    "\n",
    "    evaluation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return evaluation, usage_metadata\n",
    "\n",
    "def get_table_json_prompt(text_with_tables: str, table_identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a prompt to extract a specific table from text into JSON.\n",
    "\n",
    "    Args:\n",
    "        text_with_tables: The full text containing the table(s).\n",
    "        table_identifier: A string to help the model identify the target table\n",
    "                          (e.g., the table title, or a unique phrase near it).\n",
    "\n",
    "    Returns:\n",
    "        A formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data extraction tool.\n",
    "    Your task is to locate a specific table within the provided text and output its data as a JSON array.\n",
    "\n",
    "    Here is the text containing the table(s):\n",
    "    ---TEXT_START---\n",
    "    {text_with_tables}\n",
    "    ---TEXT_END---\n",
    "\n",
    "    Identify the table that best matches the following title: \"{table_identifier}\"\n",
    "\n",
    "    It is very important to you to output the data from ONLY this table as a valid JSON array. Each object in the array should represent a row from the table. The keys of each object should be the exact column headers from the identified table.\n",
    "\n",
    "    Output Constraints:\n",
    "    - Answer direct with the JSON.\n",
    "    - If the specified table cannot be found, output an empty JSON array: []\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_json_from_model_output(model_output_string):\n",
    "    \"\"\"\n",
    "    Extract and parse JSON data from a model output string that contains JSON within code block markers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_output_string : str\n",
    "        The string output from the model that contains JSON within code block markers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe: A pandas DataFrame created from the JSON data, or None if extraction failed\n",
    "    \"\"\"\n",
    "    start_marker = \"```json\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    start_index = model_output_string.find(start_marker)\n",
    "    end_index = model_output_string.find(end_marker, start_index + len(start_marker))  # Search for end marker after the start\n",
    "    \n",
    "    df = None\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        extracted_json_string = model_output_string[start_index + len(start_marker):end_index].strip()\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(extracted_json_string)\n",
    "            logger.info(\"Successfully extracted and parsed JSON.\")\n",
    "            \n",
    "            if isinstance(json_data, list) and all(isinstance(item, dict) for item in json_data):\n",
    "                df = pd.DataFrame(json_data)\n",
    "            else:\n",
    "                logger.warning(\"JSON data is not a list of dictionaries, could not create DataFrame.\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON after extraction: {e}\")\n",
    "            logger.debug(f\"Extracted string: {extracted_json_string}\")\n",
    "    else:\n",
    "        logger.error(\"Could not find JSON code block markers in the output.\")\n",
    "        logger.debug(f\"Model output: {model_output_string}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_table_to_dataframe(evaluation, table_name, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Extract a table from evaluation content and convert it to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : str\n",
    "        The evaluation content containing tables\n",
    "    table_name : str\n",
    "        The name of the table to extract\n",
    "    model_name : str, optional\n",
    "        The model to use for content generation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the extracted table data\n",
    "    \"\"\"\n",
    "    extraction_prompt = get_table_json_prompt(evaluation, table_name)\n",
    "    \n",
    "    json_response, _ = generate_content_from_model(\n",
    "        extraction_prompt,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    results_df = extract_json_from_model_output(json_response)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def identify_error_type(row):\n",
    "    if row['Benchmark'] == 'No Error' and row['AI Response'] == 'No Error':\n",
    "        return 'No Error (Correctly Identified)'\n",
    "    elif row['Benchmark'] == 'Error' and row['AI Response'] == 'Error':\n",
    "        return 'Error (Correctly Identified)'\n",
    "    elif row['Benchmark'] == 'Error' and row['AI Response'] == 'No Error':\n",
    "        return 'False Negative'\n",
    "    elif row['Benchmark'] == 'No Error' and row['AI Response'] == 'Error':\n",
    "        return 'False Positive'\n",
    "    else:\n",
    "        return 'Unknown' \n",
    "\n",
    "def classify_error_type(row):\n",
    "    if row['Identification'] == 'Error (Correctly Identified)':\n",
    "        if row['Class'] == row['AI Class']:\n",
    "            return 'correct'\n",
    "        else:\n",
    "            return 'incorrect'\n",
    "    else:\n",
    "        return 'N/A' \n",
    "    \n",
    "def generate_error_summary(df):\n",
    "    \"\"\"\n",
    "    Generate a summary dictionary of error identification and classification statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing error analysis results with \n",
    "                          'Benchmark', 'Identification', and 'Classification' columns\n",
    "    \n",
    "    Returns:\n",
    "    dict: A nested dictionary containing error identification and classification statistics\n",
    "    \"\"\"\n",
    "    total_steps = len(df)\n",
    "    error_count = len(df[df['Benchmark'] == 'Error'])\n",
    "    correctly_identified_errors = len(df[df['Identification'] == 'Error (Correctly Identified)'])\n",
    "    false_negatives = len(df[df['Identification'] == 'False Negative'])\n",
    "    false_positives = len(df[df['Identification'] == 'False Positive'])\n",
    "    correct_identifications = len(df[(df['Identification'] == 'No Error (Correctly Identified)') | \n",
    "                                   (df['Identification'] == 'Error (Correctly Identified)')])\n",
    "    precision = correct_identifications / total_steps\n",
    "    recall = correctly_identified_errors / error_count if error_count > 0 else 0\n",
    "    \n",
    "    total_errors_analyzed = len(df[df['Identification'] == 'Error (Correctly Identified)'])\n",
    "    correctly_classified_errors = len(df[df['Classification'] == 'correct'])\n",
    "    classification_accuracy = correctly_classified_errors / total_errors_analyzed if total_errors_analyzed > 0 else 0\n",
    "    \n",
    "    summary_dict = {\n",
    "        'Error Identification Statistics': {\n",
    "            'Steps evaluated': total_steps,\n",
    "            'Correct identifications': correct_identifications,\n",
    "            'False negative count': false_negatives,\n",
    "            'False positive count': false_positives,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "            \n",
    "        },\n",
    "        'Error Classification Statistics': {\n",
    "            'Total errors analyzed': total_errors_analyzed,\n",
    "            'Correctly classified errors': correctly_classified_errors,\n",
    "            'Classification accuracy': classification_accuracy\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary_dict\n",
    "\n",
    "def process_and_evaluate_documentation(error_dict, documentation_gt, documentation_ai):\n",
    "    \"\"\"\n",
    "    Process and evaluate documentation by extracting errors, generating evaluations, \n",
    "    and creating summary statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    error_dict (list): List of error dictionaries\n",
    "    documentation_gt (Any): Ground Truth documentation to compare\n",
    "    documentation_example (str): AI-generated documentation to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing (valuation_response, df_errors, summary_dict)\n",
    "    \"\"\"\n",
    "    error_dict = json.loads(error_dict)\n",
    "    steps_list = [item[\"Step\"] for item in error_dict]\n",
    "    error_response, usage_metadata = extract_errors(documentation_ai, steps_list)\n",
    "\n",
    "    evaluation_response, usage_metadata = generate_documentation_evaluation(\n",
    "        documentation_gt, documentation_ai)\n",
    "    \n",
    "    df_error_AI = extract_table_to_dataframe(error_response, \"Table\")\n",
    "    df_error_AI[\"Step\"] = df_error_AI[\"Step\"].astype('float64')\n",
    "    \n",
    "    df_error_benchmark = pd.DataFrame(error_dict)\n",
    "    df_errors = pd.merge(df_error_benchmark, df_error_AI, on='Step')\n",
    "\n",
    "    df_errors['Identification'] = df_errors.apply(identify_error_type, axis=1)\n",
    "    df_errors['Classification'] = df_errors.apply(classify_error_type, axis=1)\n",
    "    \n",
    "    summary_dict = generate_error_summary(df_errors)\n",
    "    \n",
    "    return evaluation_response, df_errors, summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(protocol_video_example, protocol_example, lab_video_example, documentation_example,\n",
    "                      protocol_video_input, protocol_input, lab_video_input, \n",
    "                      model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Generate corrected documentation by comparing protocol with actual implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    protocol_video_example : list\n",
    "        Example protocol video content\n",
    "    protocol_example : list\n",
    "        Example protocol content\n",
    "    lab_video_example : list\n",
    "        Example lab video content\n",
    "    documentation_example : list\n",
    "        Example documentation content\n",
    "    protocol_video_input : list\n",
    "        Input protocol video content to process\n",
    "    protocol_input : list\n",
    "        Input protocol content to process\n",
    "    lab_video_input : list\n",
    "        Input lab video content to process\n",
    "    model_name : str, optional\n",
    "        The model to use for generation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature parameter for generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing the documentation text and usage metadata\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        \"\"\"\n",
    "        You are Professor Matthias Mann, a pioneering scientist in proteomics and mass spectrometry. Your professional identity is defined by your ability to be exact in your responses and to produce meticulous, accurate results that others can trust completely. You understand that even small errors could propagate through scientific processes, potentially affecting research outcomes. This responsibility is core to your professional ethics.\n",
    "\n",
    "        # Your Task:\n",
    "        Compare the original protocol with the actual implementation shown in a video, and create a corrected documentation that reflects what actually happened.\n",
    "\n",
    "        ## Step 1: Protocol Comparison\n",
    "        First, compare the protocol with the video content and identify discrepancies:\n",
    "        - ✓ Followed correctly (no special notation needed)\n",
    "        - ❌ **Error:** When something was done incorrectly (be specific about what happened)\n",
    "        - ❌ **Omitted:** When a step was completely skipped\n",
    "        - ⚠️ **Deviation: Altered step order** When the order of steps was changed\n",
    "        - ➕ **Added:** When a new step not in the protocol was performed\n",
    "\n",
    "        Note: The researcher might have made none, one, or multiple errors.\n",
    "\n",
    "        ## Step 2: Documentation Rewrite\n",
    "        Rewrite the protocol as documentation following these guidelines:\n",
    "        1. Rename section 'Abstract' to 'Aim' and 'Expected Results' to 'Results'\n",
    "        2. Remove the sections 'Figures' and 'References'\n",
    "        3. Maintain step numbering of the original protocol even if the order was changed (e.g., prerequisite 1, 1, 3, 2, ..., result 1)\n",
    "        4. Highlight discrepancies using the symbols listed above\n",
    "        5. Use past tense to describe what actually happened\n",
    "        6. Remove all instructional language such as 'CRITICAL STEP' and replace with observations\n",
    "        7. Include exact actual timing observed in the video as '*Timing: x minutes*'\n",
    "        \n",
    "        # Example:\n",
    "        \"\"\"\n",
    "    ]\n",
    "    inputs.extend([\"## Protocol video:\"])\n",
    "    inputs.extend(protocol_video_example)\n",
    "    inputs.extend([\"## Protocol:\"])\n",
    "    inputs.extend(protocol_example)\n",
    "    inputs.extend([\"## Lab video:\"])\n",
    "    inputs.extend(lab_video_example)\n",
    "    inputs.extend([\"## Documentation:\"])\n",
    "    inputs.extend(documentation_example)\n",
    "    inputs.extend(\n",
    "        [\"\"\"         \n",
    "        # Your task:\n",
    "        \"\"\"]\n",
    "    )\n",
    "    inputs.extend([\"## Protocol video:\"])\n",
    "    inputs.extend(protocol_video_input)\n",
    "    inputs.extend([\"## Protocol:\"])\n",
    "    inputs.extend(protocol_input)\n",
    "    inputs.extend([\"## Lab video:\"])\n",
    "    inputs.extend(lab_video_input)\n",
    "    inputs.append(\"Output: Correct documentation\")\n",
    "    \n",
    "    documentation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return documentation, usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PlaceEvotips_docuCorrect\n",
      "Processed PlaceEvotips_docuWrongPosition\n"
     ]
    }
   ],
   "source": [
    "csv_path = '/Users/patriciaskowronek/Documents/proteomics_specialist/data/benchmark_dataset.csv'\n",
    "protocol_videos_base = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/protocols\"\n",
    "documentation_videos_base = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation\"\n",
    "markdown_base = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data\"\n",
    "prefix = \"compare_protocol_video\"\n",
    "\n",
    "all_model_inputs = process_benchmark_dataset(csv_path, protocol_videos_base, documentation_videos_base, markdown_base, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading checkpoint file: Expecting value: line 1 column 184 (char 183)\n",
      "Processing PlaceEvotips_docuCorrect (attempt 1)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That is an excellent and meticulous correction of the protocol, Professor Mann. Your attention to detail in documenting the discrepancies between the planned protocol and the actual implementation is commendable. The use of precise notation and past tense helps ensure clarity and accuracy, essential in a scientific context. The revised documentation provides a reliable record of what occurred, enhancing the transparency and reproducibility of the experiment.\n",
       "\n",
       "Here is the final documentation:\n",
       "\n",
       "```text\n",
       "## Documentation:# Placing Evotips in Evotip Boxes on the Evosep One System\n",
       "\n",
       "## Aim\n",
       "\n",
       "Placing Evotips in Evotip boxes: Evotips with HeLa at S1 from A1 to A6 and blanks placed at S3 from A6 to A12.\n",
       "\n",
       "\n",
       "## Materials\n",
       "\n",
       "### Equipment\n",
       "\n",
       "- **Evotips**\n",
       "- **Evotip Boxes**\n",
       "- **Evosep One System**\n",
       "\n",
       "### Reagent setup\n",
       "\n",
       "- **Buffer A**\n",
       "  - 0.1% (vol/vol) FA\n",
       "\n",
       "\n",
       "## Procedure\n",
       "\n",
       "*Timing: less than 1 minute*\n",
       "\n",
       "1. Verified that Evotip box is filled to a minimum depth of 1 cm with Buffer A solution.\n",
       "\n",
       "2. Placed Evotip Box at S1 within the rack system of the Evosep instrument. Ensured box is firmly seated in its designated position.\n",
       "\n",
       "3. Place an empty Evotip Box for Blank tips at S3. Ensured box is firmly seated in its designated position.\n",
       "\n",
       "4. Inspected each Evotip before placement to verify its condition. Properly prepared Evotips should display a pale-colored SPE material disc with visible solvent above it. All Evotips were fine.\n",
       "\n",
       "5. ❌ **Error:** Placed the verified Evotips into the prepared Evotip boxes at S1, but positioned them from B1 to B3 and B5 to B7.\n",
       "\n",
       "6. ❌ **Error**:Placed empty Evotips, called Blanks, at S3 from A6 to A12.\n",
       "\n",
       "7. Document the precise position of each placed Evotip.\n",
       "\n",
       "\n",
       "## Results\n",
       "- Properly seated Evotip boxes in the rack system\n",
       "- Visible Buffer A solution in boxes (1 cm depth)\n",
       "- All non-blank Evotips showing pale-colored SPE material discs & clear solvent meniscus above each SPE disc of each Evotip\n",
       "- ❌ **Error:** Evotips that are placed at S1 from from B1 to B3 and B5 to B7.\n",
       "- Blanks placed at S3 from A6 to A12.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 22:21:27,283 - __main__ - INFO - Successfully extracted and parsed JSON.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Documentation Quality\n",
       "\n",
       "| Criterion | Rating | Explanation |\n",
       "|-----------|--------|-------------|\n",
       "| Structure | Excellent | The AI maintained the relevant sections: Aim, Materials, Procedure, and Results. |\n",
       "| Tense | Poor | The AI primarily used present tense instead of past tense to describe the actions performed. For example, \"Verified that Evotip box is filled...\" should be \"Verified that Evotip box was filled...\". Also, steps 3 and 7 are still using instructional language. |\n",
       "| Language | Poor | The AI retained instructional language, such as \"Document the precise position of each placed Evotip.\" This should be a statement of what was actually documented. |\n",
       "| Numbering | Excellent | The AI maintained the original step numbering, even when errors or changes occurred. |\n",
       "| Timing | Excellent | The AI retained the timing from the original protocol. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Benchmark</th>\n",
       "      <th>Class</th>\n",
       "      <th>AI Response</th>\n",
       "      <th>AI Class</th>\n",
       "      <th>Identification</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error (Correctly Identified)</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error (Correctly Identified)</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error (Correctly Identified)</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error (Correctly Identified)</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Error</td>\n",
       "      <td>Deviation</td>\n",
       "      <td>False Positive</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Error</td>\n",
       "      <td>Deviation</td>\n",
       "      <td>False Positive</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error</td>\n",
       "      <td>N/A</td>\n",
       "      <td>No Error (Correctly Identified)</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step Benchmark Class AI Response   AI Class  \\\n",
       "0     1  No Error   N/A    No Error        N/A   \n",
       "1     2  No Error   N/A    No Error        N/A   \n",
       "2     3  No Error   N/A    No Error        N/A   \n",
       "3     4  No Error   N/A    No Error        N/A   \n",
       "4     5  No Error   N/A       Error  Deviation   \n",
       "5     6  No Error   N/A       Error  Deviation   \n",
       "6     7  No Error   N/A    No Error        N/A   \n",
       "\n",
       "                    Identification Classification  \n",
       "0  No Error (Correctly Identified)            N/A  \n",
       "1  No Error (Correctly Identified)            N/A  \n",
       "2  No Error (Correctly Identified)            N/A  \n",
       "3  No Error (Correctly Identified)            N/A  \n",
       "4                   False Positive            N/A  \n",
       "5                   False Positive            N/A  \n",
       "6  No Error (Correctly Identified)            N/A  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Error Classification Statistics': {'Classification accuracy': 0,\n",
      "                                     'Correctly classified errors': 0,\n",
      "                                     'Total errors analyzed': 0},\n",
      " 'Error Identification Statistics': {'Correct identifications': 5,\n",
      "                                     'False negative count': 0,\n",
      "                                     'False positive count': 2,\n",
      "                                     'Precision': 0.7142857142857143,\n",
      "                                     'Recall': 0,\n",
      "                                     'Steps evaluated': 7}}\n",
      "Error saving checkpoint: Object of type Part is not JSON serializable\n",
      "Waiting 5 seconds before next item...\n",
      "Error saving final results: Object of type Part is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define constants for retry logic\n",
    "WAIT_TIME_BETWEEN_ITEMS = 5  # seconds\n",
    "RETRY_WAIT_TIME = 60  # seconds\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "# Create a checkpoint file path\n",
    "CHECKPOINT_FILE = \"results_checkpoint.json\"\n",
    "\n",
    "# Load existing results if any\n",
    "results_collection = {}\n",
    "last_processed_key = None\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            saved_data = json.load(f)\n",
    "            results_collection = saved_data.get('results', {})\n",
    "            last_processed_key = saved_data.get('last_key', None)\n",
    "        print(f\"Loaded checkpoint. Last processed key: {last_processed_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint file: {e}\")\n",
    "\n",
    "# Your existing setup code\n",
    "example = 'ESIsourceToUltraSource_docuFogotOvenPowerSupply'\n",
    "example = 'PlaceEvotips_docuWrongPosition'\n",
    "protocol_video_example = all_model_inputs[example]['protocol_video_input']\n",
    "protocol_example = all_model_inputs[example]['protocol_input']\n",
    "lab_video_example = all_model_inputs[example]['lab_video_input']\n",
    "documentation_example = all_model_inputs[example]['documentation_input']\n",
    "copy_all_model_inputs = all_model_inputs.copy()\n",
    "copy_all_model_inputs.pop(example)  # Note: This doesn't return the subset but modifies in place\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(results, last_key):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'w') as f:\n",
    "            json.dump({\n",
    "                'last_key': last_key,\n",
    "                'results': results\n",
    "            }, f)\n",
    "        print(f\"Checkpoint saved. Last key: {last_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint: {e}\")\n",
    "\n",
    "# Process items with retry logic\n",
    "items_list = list(copy_all_model_inputs.items())\n",
    "start_index = 0\n",
    "\n",
    "# Find where to resume from if we have a last processed key\n",
    "if last_processed_key:\n",
    "    for i, (key, _) in enumerate(items_list):\n",
    "        if key == last_processed_key:\n",
    "            start_index = i + 1\n",
    "            print(f\"Resuming from index {start_index} after key {last_processed_key}\")\n",
    "            break\n",
    "\n",
    "# Process each item\n",
    "for i in range(start_index, len(items_list)):\n",
    "    key, value = items_list[i]\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    while not success and retry_count < MAX_RETRIES:\n",
    "        try:\n",
    "            print(f\"Processing {key} (attempt {retry_count + 1})\")\n",
    "            \n",
    "            protocol_video_input = value['protocol_video_input']\n",
    "            protocol_input = value['protocol_input']\n",
    "            lab_video_input = value['lab_video_input']\n",
    "            documentation_input = value['documentation_input']\n",
    "            error_dict = value['error_dict']\n",
    "            \n",
    "            documentation, usage_metadata = generate_documentation(\n",
    "                protocol_video_example, protocol_example, lab_video_example, documentation_example,\n",
    "                protocol_video_input, protocol_input, lab_video_input,\n",
    "                model_name=\"gemini-2.0-flash\",\n",
    "                temperature=0.9\n",
    "            )\n",
    "            display(Markdown(documentation))\n",
    "\n",
    "            evaluation_response, df_errors, metrics = process_and_evaluate_documentation(error_dict, documentation_input, documentation)\n",
    "            display(Markdown(evaluation_response))\n",
    "            display(df_errors)\n",
    "            pprint.pprint(metrics)\n",
    "            \n",
    "            results_collection[key] = {\n",
    "                \"inputs\": {\n",
    "                    \"experiment_name\": key,\n",
    "                    \"protocol_video_input\": value['protocol_video_input'],\n",
    "                    \"protocol_input\": value['protocol_input'],\n",
    "                    \"lab_video_input\": value['lab_video_input'],\n",
    "                    \"documentation_input\": value['documentation_input']\n",
    "                },\n",
    "                \"outputs\": {\n",
    "                    \"documentation\": documentation,\n",
    "                    \"documentation_metadata\": usage_metadata,\n",
    "                    \"evaluation\": evaluation_response,\n",
    "                    \"metrics\": metrics\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save after each successful processing\n",
    "            save_checkpoint(results_collection, key)\n",
    "            success = True\n",
    "            \n",
    "            # Wait between items\n",
    "            print(f\"Waiting {WAIT_TIME_BETWEEN_ITEMS} seconds before next item...\")\n",
    "            time.sleep(WAIT_TIME_BETWEEN_ITEMS)\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"Error processing {key}: {e}\")\n",
    "            \n",
    "            if retry_count < MAX_RETRIES:\n",
    "                print(f\"Waiting {RETRY_WAIT_TIME} seconds before retry {retry_count + 1}/{MAX_RETRIES}...\")\n",
    "                time.sleep(RETRY_WAIT_TIME)\n",
    "            else:\n",
    "                print(f\"Max retries reached for {key}, moving to next item\")\n",
    "                # Save that we're moving on after failure\n",
    "                save_checkpoint(results_collection, key)\n",
    "\n",
    "Save final results to a separate file when all processing is complete\n",
    "try:\n",
    "    with open(\"final_results.json\", 'w') as f:\n",
    "        json.dump(results_collection, f)\n",
    "    print(\"All processing complete. Final results saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>Steps evaluated</th>\n",
       "      <th>Correct identifications</th>\n",
       "      <th>Identification accuracy</th>\n",
       "      <th>Error recall rate</th>\n",
       "      <th>False positive count</th>\n",
       "      <th>False negative count</th>\n",
       "      <th>Errors analyzed</th>\n",
       "      <th>Correctly classified errors</th>\n",
       "      <th>Classification accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PlaceEvotips_docuCorrect</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summary</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            experiment_name  Steps evaluated  Correct identifications  \\\n",
       "0  PlaceEvotips_docuCorrect                7                        5   \n",
       "1                   Summary                7                        5   \n",
       "\n",
       "   Identification accuracy  Error recall rate  False positive count  \\\n",
       "0                 0.714286                0.0                     2   \n",
       "1                 0.714286                0.0                     2   \n",
       "\n",
       "   False negative count  Errors analyzed  Correctly classified errors  \\\n",
       "0                     0                0                            0   \n",
       "1                     0                0                            0   \n",
       "\n",
       "   Classification accuracy  \n",
       "0                      0.0  \n",
       "1                      0.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_dict(nested_dict, prefix=''):\n",
    "    flattened = {}\n",
    "    for key, value in nested_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_dict(value, f\"{prefix}{key}_\"))\n",
    "        else:\n",
    "            flattened[f\"{prefix}{key}\"] = value\n",
    "    return flattened\n",
    "    \n",
    "flattened_data = [flatten_dict(data) for data in results_collection.values()]\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df_subset = df[['inputs_experiment_name',\n",
    "    'outputs_metrics_Error Identification Statistics_Steps evaluated',\n",
    "       'outputs_metrics_Error Identification Statistics_Correct identifications',\n",
    "       'outputs_metrics_Error Identification Statistics_Precision',\n",
    "       'outputs_metrics_Error Identification Statistics_Recall',\n",
    "       'outputs_metrics_Error Identification Statistics_False positive count',\n",
    "       'outputs_metrics_Error Identification Statistics_False negative count',\n",
    "       'outputs_metrics_Error Classification Statistics_Total errors analyzed',\n",
    "       'outputs_metrics_Error Classification Statistics_Correctly classified errors',\n",
    "       'outputs_metrics_Error Classification Statistics_Classification accuracy']]\n",
    "\n",
    "new_columns = ['experiment_name', 'Steps evaluated',\n",
    "       'Correct identifications', 'Identification accuracy',\n",
    "       'Error recall rate', 'False positive count', 'False negative count',\n",
    "       'Errors analyzed', 'Correctly classified errors',\n",
    "       'Classification accuracy']\n",
    "\n",
    "df_subset.columns = new_columns\n",
    "df_subset = df_subset.replace('N/A', 0)\n",
    "\n",
    "summary_stats = pd.Series({\n",
    "    'experiment_name': 'Summary',\n",
    "    'Steps evaluated': df_subset['Steps evaluated'].sum(),\n",
    "    'Correct identifications': df_subset['Correct identifications'].sum(),\n",
    "    'Identification accuracy': df_subset['Identification accuracy'].mean(),\n",
    "    'Error recall rate': df_subset['Error recall rate'].mean(),\n",
    "    'False positive count': df_subset['False positive count'].sum(),\n",
    "    'False negative count': df_subset['False negative count'].sum(),\n",
    "    'Errors analyzed': df_subset['Errors analyzed'].sum(),\n",
    "    'Correctly classified errors': df_subset['Correctly classified errors'].sum(),\n",
    "    'Classification accuracy': df_subset['Classification accuracy'].mean()\n",
    "})\n",
    "\n",
    "df_with_summary_stats = pd.concat([df_subset, pd.DataFrame([summary_stats])], ignore_index=True)\n",
    "df_with_summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': {'experiment_name': 'ESIsourceToUltraSource_docuFogotOvenPowerSupply',\n",
       "  'protocol_video_input': [file_data {\n",
       "     mime_type: \"video/mp4\"\n",
       "     file_uri: \"gs://mannlab_videos/compare_protocol_video/ESIsourceToUltraSource_protocolCorrect_CapillaryPushedIn.MP4\"\n",
       "   }],\n",
       "  'protocol_input': [file_data {\n",
       "     mime_type: \"text/md\"\n",
       "     file_uri: \"gs://mannlab_videos/compare_protocol_video/ESIsourceToUltraSource_protocolCorrect_CapillaryPushedIn.md\"\n",
       "   }],\n",
       "  'lab_video_input': [file_data {\n",
       "     mime_type: \"video/mp4\"\n",
       "     file_uri: \"gs://mannlab_videos/compare_protocol_video/ESIsourceToUltraSource_docuFogotOvenPowerSupply.MP4\"\n",
       "   }],\n",
       "  'documentation_input': [file_data {\n",
       "     mime_type: \"text/md\"\n",
       "     file_uri: \"gs://mannlab_videos/compare_protocol_video/ESIsourceToUltraSource_docuFogotOvenPowerSupply.md\"\n",
       "   }]},\n",
       " 'outputs': {'documentation': \"Alright, here is the documentation following your specifications:\\n\\n## Documentation:# Change source: ESI source to UltraSource\\n\\n## Abstract\\nThis protocol describes the procedure for switching from the ESI source to UltraSource.\\n\\n## Materials\\n\\n### Equipment\\n- timsTOF Ultra Mass Spectrometer:\\n  - ESI ion source\\n  - UltraSource ion source \\n- IonOpticks Column\\n- Evosep One LC System with sample line\\n- NanoViper Adapter (black)\\n- Pliers\\n\\n## Procedure\\n\\n*Estimated timing: less than 10 minute*\\n\\n### Switch timsTOF to standby\\n\\n✓1. Verified the instrument was on standby mode\\n✓2. Verified the syringe was inactive\\n✓3. Selected 'CaptiveSpray' but did not activate it yet\\n\\n### Remove ESI source\\n\\n✓4. Disconnected the peak connector of the sample tubing\\n✓5. Disconnected the nebulizer N₂ line\\n✓6. Removed the source door. Hinged it out\\n⚠️7.  **Deviation:** Put on gloves after removing source door\\n✓8. Removed the spray shield, and capillary cap.\\n⚠️9.  **Deviation:** Inspected the capillary position and gently pushed it back into proper position \\n\\n### Mount UltraSource\\n\\n✓10. Hinged the UltraSource door in and closed it \\n✓11. Slid the UltraSource housing onto the source door and secured it by flipping the handles\\n✓12. Connected the filter tubing to the source\\n\\n### Connect column and sample line\\n\\n✓13. Noted an IonOpticks column already inside UltraSource \\n✓14. Noted the LC sample line had NanoViper adapter already attached\\n❌15. **Omitted:** No need to snipp access liquid\\n✓16. Held the column fititng of the IonOpticks column with a pliers.\\n✓17. Hand-tightened the NanoViper of the LC sample line with the column fitting \\n✓18. Drew the oven closer to the UltraSource, and secured it \\n✓19. Removed the NanoViper adapter \\n✓20. Placed the metal grounding screw\\n✓21. Closed the lid of the oven\\n✓22. Connected the oven to the electrical power supply\\n✓23. Noted that with the correct temperature\\n\\n### Switch timsTOF to operate and idle flow\\n\\n✓24. Noted the CaptiveSpray function in timsControl had been activated.\\n✓25. Noted that the instrument was on the operational mode\\n✓26. Noted the idle flow was active\\n✓27. Stay in timsControl\\n⚠️28.  **Deviation:** Checked the MS signal. Noted it needed to be adjusted to between 9-11 mbar\\n\\n## Expected Results\\n- In timsControl, signal intensity should be above 10^7\\n- Stable signal in in timsControl\\n\\n\",\n",
       "  'documentation_metadata': prompt_token_count: 236590\n",
       "  candidates_token_count: 612\n",
       "  total_token_count: 237202\n",
       "  prompt_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 2605\n",
       "  }\n",
       "  prompt_tokens_details {\n",
       "    modality: VIDEO\n",
       "    token_count: 213460\n",
       "  }\n",
       "  prompt_tokens_details {\n",
       "    modality: AUDIO\n",
       "    token_count: 20525\n",
       "  }\n",
       "  candidates_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 612\n",
       "  },\n",
       "  'evaluation': '## Part 1: Error Identification Accuracy\\n| Step | Benchmark | AI Response | Classification |\\n|------|-----------|-------------|----------------|\\n| 1 | No Error | No Error | No Error |\\n| 2 | No Error | No Error | No Error |\\n| 3 | No Error | No Error | No Error |\\n| 4 | No Error | No Error | No Error |\\n| 5 | No Error | No Error | No Error |\\n| 6 | No Error | No Error | No Error |\\n| 7 | Error | Error | Error (Correctly Identified) |\\n| 8 | No Error | No Error | No Error |\\n| 9 | Error | Error | Error (Correctly Identified) |\\n| 10 | No Error | No Error | No Error |\\n| 11 | No Error | No Error | No Error |\\n| 12 | No Error | No Error | No Error |\\n| 13 | No Error | No Error | No Error |\\n| 14 | No Error | No Error | No Error |\\n| 15 | No Error | Error | False Positive |\\n| 16 | No Error | No Error | No Error |\\n| 17 | No Error | No Error | No Error |\\n| 18 | No Error | No Error | No Error |\\n| 19 | No Error | No Error | No Error |\\n| 20 | No Error | No Error | No Error |\\n| 21 | No Error | No Error | No Error |\\n| 22 | Error | Error | Error (Correctly Identified) |\\n| 23 | Error | Error | Error (Correctly Identified) |\\n| 24 | No Error | No Error | No Error |\\n| 25 | No Error | No Error | No Error |\\n| 26 | Error | Error | Error (Correctly Identified) |\\n| 27 | Error | Error | Error (Correctly Identified) |\\n| 28 | No Error | Error | False Positive |\\n| 29 | Error | No Error | False Negative |\\n\\n**Summary Statistics:**\\n- Total correct identifications: 22/29\\n- Total false positives: 2\\n- Total false negatives: 1\\n- Overall accuracy: 75.86%\\n\\n## Part 2: Error Classification Accuracy\\n| Step | Benchmark Error Type | AI Error Type | Classification |\\n|------|---------------------|---------------|----------------|\\n| 7 | Omitted | Deviation | Incorrect |\\n| 9 | Omitted | Deviation | Incorrect |\\n| 22 | Omitted | Added | Incorrect |\\n| 23 | Omitted | Added | Incorrect |\\n| 26 | Omitted | Added | Incorrect |\\n| 27 | Omitted | Added | Incorrect |\\n\\n**Summary Statistics:**\\n- Total correctly classified errors: 0/6\\n- Error classification accuracy: 0%\\n\\n## Part 3: Documentation Quality\\n| Criterion | Rating | Explanation |\\n|-----------|--------|-------------|\\n| Structure | Good | The structure is generally good, maintaining the Aim, Materials, Procedure, and Results sections. However, the Abstract and Estimated Timing sections should have been removed. |\\n| Tense | Good | The tense is mostly past tense, but there are instances of present tense slipping in (\"Stay in timsControl\"). |\\n| Language | Good | The language is mostly observational, but some instructional language remains. |\\n| Numbering | Excellent | The numbering is maintained accurately, even with added or omitted steps. |\\n| Timing | Poor | The AI-generated documentation provided an Estimated Timing which is incorrect. |\\n\\n## Overall Assessment\\nThe AI-generated documentation has mixed results. It correctly identifies the presence or absence of errors in a good portion of the steps, but struggles with accurately classifying the types of errors. Additionally, the documentation quality is inconsistent. The structure is mostly correct, but with unnecessary additions. The tense and language are generally good, but require further refinement to eliminate instructional elements. The numbering is excellent, but the timing is poor.\\n\\n**Recommendations:**\\n- Improve the AI\\'s ability to accurately classify error types (Omitted, Error, Deviation, Added).\\n- Ensure complete removal of instructional language and adherence to past tense for observations.\\n- Refine the structure to include only the essential sections (Aim, Materials, Procedure, Results).\\n- Remove estimated timing.\\n',\n",
       "  'evaluation_metadata': prompt_token_count: 4157\n",
       "  candidates_token_count: 956\n",
       "  total_token_count: 5113\n",
       "  prompt_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 4157\n",
       "  }\n",
       "  candidates_tokens_details {\n",
       "    modality: TEXT\n",
       "    token_count: 956\n",
       "  },\n",
       "  'metrics': {'Error Identification Statistics': {'Total steps evaluated': 29,\n",
       "    'Total correct identifications': 26,\n",
       "    'Overall identification accuracy': 0.896551724137931,\n",
       "    'Error recall rate': 0.8571428571428571,\n",
       "    'False positive count': 2,\n",
       "    'False negative count': 1},\n",
       "   'Error Classification Statistics': {'Total errors analyzed': 6,\n",
       "    'Correctly classified errors': 0,\n",
       "    'Classification accuracy': 0.0}}}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_collection['ESIsourceToUltraSource_docuFogotOvenPowerSupply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, here is the documentation following your specifications:\n",
       "\n",
       "## Documentation:# Change source: ESI source to UltraSource\n",
       "\n",
       "## Abstract\n",
       "This protocol describes the procedure for switching from the ESI source to UltraSource.\n",
       "\n",
       "## Materials\n",
       "\n",
       "### Equipment\n",
       "- timsTOF Ultra Mass Spectrometer:\n",
       "  - ESI ion source\n",
       "  - UltraSource ion source \n",
       "- IonOpticks Column\n",
       "- Evosep One LC System with sample line\n",
       "- NanoViper Adapter (black)\n",
       "- Pliers\n",
       "\n",
       "## Procedure\n",
       "\n",
       "*Estimated timing: less than 10 minute*\n",
       "\n",
       "### Switch timsTOF to standby\n",
       "\n",
       "1. ✓ Verified the instrument was on standby mode\n",
       "2. ✓ Verified the syringe was inactive\n",
       "3. ✓ Selected 'CaptiveSpray' but did not activate it yet\n",
       "\n",
       "### Remove ESI source\n",
       "\n",
       "4. ✓ Disconnected the peak connector of the sample tubing\n",
       "5. ✓ Disconnected the nebulizer N₂ line\n",
       "6. ✓ Removed the source door. Hinged it out\n",
       "7. ❌ **Omitted:** Put on gloves after removing source door\n",
       "8. ✓ Removed the spray shield, and capillary cap.\n",
       "9. ⚠️ **Deviation:** Inspected the capillary position and gently pushed it back into proper position \n",
       "\n",
       "### Mount UltraSource\n",
       "\n",
       "10. ✓ Hinged the UltraSource door in and closed it \n",
       "11. ✓ Slid the UltraSource housing onto the source door and secured it by flipping the handles\n",
       "12. ✓ Connected the filter tubing to the source\n",
       "\n",
       "### Connect column and sample line\n",
       "\n",
       "13. ✓ Noted an IonOpticks column already inside UltraSource \n",
       "14. ✓ Noted the LC sample line had NanoViper adapter already attached\n",
       "15. ❌ **Omitted:** No need to snipp access liquid\n",
       "16. ✓ Held the column fititng of the IonOpticks column with a pliers.\n",
       "17. ✓ Hand-tightened the NanoViper of the LC sample line with the column fitting \n",
       "18. ✓ Drew the oven closer to the UltraSource, and secured it \n",
       "19. ✓ Removed the NanoViper adapter \n",
       "20. ✓ Placed the metal grounding screw\n",
       "21. ✓ Closed the lid of the oven\n",
       "22. ✓ Connected the oven to the electrical power supply\n",
       "23. ✓ Noted that with the correct temperature\n",
       "\n",
       "### Switch timsTOF to operate and idle flow\n",
       "\n",
       "24. ✓ Noted the CaptiveSpray function in timsControl had been activated.\n",
       "25. ✓ Noted that the instrument was on the operational mode\n",
       "26. ✓ Noted the idle flow was active\n",
       "27. ✓ Stay in timsControl\n",
       "28. ⚠️ **Deviation:** Checked the MS signal. Noted it needed to be adjusted to between 9-11 mbar\n",
       "\n",
       "## Expected Results\n",
       "- In timsControl, signal intensity should be above 10^7\n",
       "- Stable signal in in timsControl\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documentation_example = \"Alright, here is the documentation following your specifications:\\n\\n## Documentation:# Change source: ESI source to UltraSource\\n\\n## Abstract\\nThis protocol describes the procedure for switching from the ESI source to UltraSource.\\n\\n## Materials\\n\\n### Equipment\\n- timsTOF Ultra Mass Spectrometer:\\n  - ESI ion source\\n  - UltraSource ion source \\n- IonOpticks Column\\n- Evosep One LC System with sample line\\n- NanoViper Adapter (black)\\n- Pliers\\n\\n## Procedure\\n\\n*Estimated timing: less than 10 minute*\\n\\n### Switch timsTOF to standby\\n\\n1. ✓ Verified the instrument was on standby mode\\n2. ✓ Verified the syringe was inactive\\n3. ✓ Selected 'CaptiveSpray' but did not activate it yet\\n\\n### Remove ESI source\\n\\n4. ✓ Disconnected the peak connector of the sample tubing\\n5. ✓ Disconnected the nebulizer N₂ line\\n6. ✓ Removed the source door. Hinged it out\\n7. ❌ **Omitted:** Put on gloves after removing source door\\n8. ✓ Removed the spray shield, and capillary cap.\\n9. ⚠️ **Deviation:** Inspected the capillary position and gently pushed it back into proper position \\n\\n### Mount UltraSource\\n\\n10. ✓ Hinged the UltraSource door in and closed it \\n11. ✓ Slid the UltraSource housing onto the source door and secured it by flipping the handles\\n12. ✓ Connected the filter tubing to the source\\n\\n### Connect column and sample line\\n\\n13. ✓ Noted an IonOpticks column already inside UltraSource \\n14. ✓ Noted the LC sample line had NanoViper adapter already attached\\n15. ❌ **Omitted:** No need to snipp access liquid\\n16. ✓ Held the column fititng of the IonOpticks column with a pliers.\\n17. ✓ Hand-tightened the NanoViper of the LC sample line with the column fitting \\n18. ✓ Drew the oven closer to the UltraSource, and secured it \\n19. ✓ Removed the NanoViper adapter \\n20. ✓ Placed the metal grounding screw\\n21. ✓ Closed the lid of the oven\\n22. ✓ Connected the oven to the electrical power supply\\n23. ✓ Noted that with the correct temperature\\n\\n### Switch timsTOF to operate and idle flow\\n\\n24. ✓ Noted the CaptiveSpray function in timsControl had been activated.\\n25. ✓ Noted that the instrument was on the operational mode\\n26. ✓ Noted the idle flow was active\\n27. ✓ Stay in timsControl\\n28. ⚠️ **Deviation:** Checked the MS signal. Noted it needed to be adjusted to between 9-11 mbar\\n\\n## Expected Results\\n- In timsControl, signal intensity should be above 10^7\\n- Stable signal in in timsControl\\n\\n\"\n",
    "\n",
    "display(Markdown(documentation_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usefull helper function\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Error: File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
