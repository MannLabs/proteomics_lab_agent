{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation assistant\n",
    "\n",
    "This notebook demonstrates a documentation assistant: Video-to-documentation conversion using Vertex AI\n",
    "\n",
    "Converting videos-to-documentation involves three steps: \n",
    "1. Protocol finder: Select protocol which best captures the step being performed in the video\n",
    "2. Video comparing to ground-of-truth protocol → lab documentation + errors in procedure\n",
    "3. Analytics based on benchmark dataset: automatic comparison of errors found by documentation assistent vs actual errors\n",
    "\n",
    "In this notebook, I will focus on the step two and three - Compare video with protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../secrets.ini']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "path_to_append = Path(Path.cwd()).parent / \"proteomics_specialist\"\n",
    "sys.path.append(str(path_to_append))\n",
    "import video_to_protocol\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../secrets.ini\")\n",
    "\n",
    "PROJECT_ID = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "vertexai.init(project=PROJECT_ID, location=\"europe-west9\")  # europe-west9 is Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = config[\"DEFAULT\"][\"PROJECT_ID\"]\n",
    "\n",
    "# Initialize Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = \"mannlab_videos\"\n",
    "bucket = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "from typing import TYPE_CHECKING, NamedTuple\n",
    "\n",
    "def generate_content_from_model(\n",
    "    inputs: Any,\n",
    "    model_name: str = \"gemini-2.0-flash\",\n",
    "    temperature: float = 0.9,\n",
    ") -> tuple:\n",
    "    \"\"\"Generate content using Google's Generative AI model.\n",
    "    \n",
    "    This function sends inputs to a specified Gemini model and returns the \n",
    "    generated response along with usage metadata.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : Any\n",
    "        The inputs to send to the model (text, images, or videos).\n",
    "    model_name : str, default=\"gemini-2.0-flash\"\n",
    "        Name of the generative model to use.\n",
    "    temperature : float, default=0.9\n",
    "        Controls the randomness of the output. Higher values (closer to 1.0)\n",
    "        make output more random, lower values make it more deterministic.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing (response_text, usage_metadata)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the model fails to generate content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = GenerativeModel(model_name)\n",
    "        \n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            # Uncomment if using single audio/video input\n",
    "            # audio_timestamp=True\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(\n",
    "            inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        documentation = response.text\n",
    "        usage_metadata = response.usage_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error during content generation\")\n",
    "        raise ValueError(f\"Failed to generate content: {str(e)}\")\n",
    "    \n",
    "    return documentation, usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import Part\n",
    "\n",
    "def prepare_all_inputs(\n",
    "    lab_video_path: str,\n",
    "    protocol_path: str,\n",
    "    documentation_video_path: str,\n",
    "    documentation_path: str,\n",
    "    bucket: str,\n",
    "    prefix: str = \"compare_protocol_video\"\n",
    ") -> dict:\n",
    "    \"\"\"Prepare all four standard inputs for the generative model.\n",
    "    \n",
    "    This function uploads the four standard files (lab video, protocol document, \n",
    "    documentation video, and documentation document) and formats them as inputs \n",
    "    for a generative model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lab_video_path : str\n",
    "        Path to the lab video file.\n",
    "    protocol_path : str\n",
    "        Path to the protocol markdown file.\n",
    "    documentation_video_path : str\n",
    "        Path to the documentation video file.\n",
    "    documentation_path : str\n",
    "        Path to the documentation markdown file.\n",
    "    bucket : str\n",
    "        GCS bucket name for uploading the files.\n",
    "    prefix : str, default=\"compare_protocol_video\"\n",
    "        Prefix for the files in GCS bucket.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the four formatted inputs:\n",
    "        'protocol_video_input', 'protocol_input', 'lab_video_input', 'documentation_input'\n",
    "    \"\"\"\n",
    "    \n",
    "    video_uri = video_to_protocol.upload_video_to_gcs(lab_video_path, bucket, prefix)\n",
    "    protocol_video_input = [Part.from_uri(video_uri, mime_type=\"video/mp4\")]\n",
    "    \n",
    "    uri = video_to_protocol.upload_video_to_gcs(protocol_path, bucket, prefix)\n",
    "    protocol_input = [Part.from_uri(uri, mime_type=\"text/md\")]\n",
    "    \n",
    "    video_uri = video_to_protocol.upload_video_to_gcs(documentation_video_path, bucket, prefix)\n",
    "    lab_video_input = [Part.from_uri(video_uri, mime_type=\"video/mp4\")]\n",
    "\n",
    "    uri = video_to_protocol.upload_video_to_gcs(documentation_path, bucket, prefix)\n",
    "    documentation_input = [Part.from_uri(uri, mime_type=\"text/md\")]\n",
    "    \n",
    "    return {\n",
    "        'protocol_video_input': protocol_video_input,\n",
    "        'protocol_input': protocol_input,\n",
    "        'lab_video_input': lab_video_input,\n",
    "        'documentation_input': documentation_input\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_benchmark_dataset(csv_path, protocol_videos_base, documentation_videos_base, markdown_base, bucket, prefix):\n",
    "    \"\"\"\n",
    "    Process the first two rows in the benchmark dataset CSV and prepare model inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing benchmark dataset information\n",
    "    protocol_videos_base : str\n",
    "        Base path to the protocol videos directory\n",
    "    documentation_videos_base : str\n",
    "        Base path to the documentation videos directory\n",
    "    markdown_base : str\n",
    "        Base path to the markdown files directory\n",
    "    bucket : object\n",
    "        The bucket object used in the prepare_all_inputs function\n",
    "    prefix : str\n",
    "        Prefix for the files in GCS bucket.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing all model inputs for the first two rows in the CSV,\n",
    "        with experiment names as keys\n",
    "    \"\"\"\n",
    "    benchmark_df = pd.read_csv(\n",
    "        csv_path, \n",
    "        sep=';'\n",
    "    )\n",
    "    \n",
    "    all_model_inputs = {}\n",
    "    \n",
    "    for index, row in benchmark_df.iterrows(): # for testing .head(2).iterrows()\n",
    "        lab_video_path = os.path.join(protocol_videos_base, row[\"protocol video\"])\n",
    "        protocol_path = os.path.join(markdown_base, row[\"protocol\"])\n",
    "        documentation_video_path = os.path.join(documentation_videos_base, row[\"documentation video\"])\n",
    "        documentation_path = os.path.join(markdown_base, row[\"documentation\"])\n",
    "        \n",
    "        dict_model_inputs = prepare_all_inputs(\n",
    "            lab_video_path,\n",
    "            protocol_path,\n",
    "            documentation_video_path,\n",
    "            documentation_path,\n",
    "            bucket,\n",
    "            prefix\n",
    "        )\n",
    "        \n",
    "        experiment_name = row[\"documentation\"].split(\".\")[0]\n",
    "        all_model_inputs[experiment_name] = dict_model_inputs\n",
    "        \n",
    "        print(f\"Processed {experiment_name}\")\n",
    "        \n",
    "    return all_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation_evaluation(documentation_input, documentation, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Generate an evaluation of AI-generated documentation against benchmark documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documentation_input : list\n",
    "        The benchmark documentation (ground truth) represented as a list of strings\n",
    "    documentation : list\n",
    "        The AI-generated documentation to evaluate represented as a list of strings\n",
    "    model_name : str, optional\n",
    "        The model to use for evaluation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing (evaluation_text, usage_metadata)\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        \"\"\"\n",
    "        # Instruction\n",
    "        You are an expert evaluator specializing in scientific protocol documentation. Your task is to evaluate the error identification accuracy, error type classification and documentation quality of an AI-generated documentation against a benchmark documentation (ground truth). You will be provided with an AI-generated documentation and a benchmark documentation (human-verified ground truth).\n",
    "\n",
    "        # Evaluation Parts\n",
    "        ## Part 1: Error Identification Accuracy\n",
    "        For each step in the protocol, determine if the AI correctly identified the presence or absence of errors by classifying into one of these categories:\n",
    "        - **No Error**: Both benchmark and AI response agree there was no error\n",
    "        - **Error (Correctly Identified)**: Both benchmark and AI response agree there was an error\n",
    "        - **False Positive**: AI response claimed an error when the benchmark indicates none\n",
    "        - **False Negative**: AI response missed an error that the benchmark shows\n",
    "\n",
    "        ## Part 2: Error Type Classification\n",
    "        For each error that was correctly identified by both the benchmark and AI response, determine if the AI correctly classified the error type:\n",
    "        - **Correct Classification**: AI used the same error type as the benchmark (Omitted, Error, Deviation, Added)\n",
    "        - **Incorrect Classification**: AI used a different error type than the benchmark\n",
    "\n",
    "        ## Part 3: Documentation Quality\n",
    "        Evaluate the AI's documentation quality based on these criteria:\n",
    "        1. **Structure**: Did it keep only relevant sections: Aim, Materials, Procedure, Results?\n",
    "        2. **Tense**: Did it use past tense to describe what actually happened, not what should happen?\n",
    "        3. **Language**: Did it remove all instructional language and replace with observations?\n",
    "        4. **Numbering**: Did it maintain step numbering of the original protocol even if order changed?\n",
    "        5. **Timing**: Did it include exact actual timing, not estimated timing?\n",
    "\n",
    "        # Rating Rubric\n",
    "        For each part, provide an evaluation:\n",
    "\n",
    "        ### Part 1: Error Identification Accuracy\n",
    "        - Calculate and report:\n",
    "            - Total number of correct identifications (No Error + Correctly Identified Error)\n",
    "            - Total number of false positives\n",
    "            - Total number of false negatives\n",
    "            - Overall accuracy percentage (correct identifications / total steps)\n",
    "\n",
    "        ### Part 2: Error Type Classification\n",
    "        - Calculate and report:\n",
    "            - Total errors correctly classified / Total errors correctly identified\n",
    "            - Overall error classification accuracy percentage\n",
    "\n",
    "        ### Part 3: Documentation Quality\n",
    "        For each criterion:\n",
    "        - **Excellent**: The criterion was fully met with no issues\n",
    "        - **Good**: The criterion was mostly met with minor issues\n",
    "        - **Poor**: The criterion was not met or had significant issues\n",
    "\n",
    "        # Evaluation Steps\n",
    "        1. Create a table for each step in the protocol showing error identification accuracy\n",
    "        2. Analyze correctly identified errors to determine classification accuracy\n",
    "        3. Evaluate documentation quality against the 5 criteria\n",
    "        4. Provide final scores and overall assessment\n",
    "        5. Highlight specific strengths and areas for improvement\n",
    "\n",
    "        # Output Format\n",
    "        ## Part 1: Error Identification Accuracy\n",
    "        | Step | Benchmark | AI Response | Classification |\n",
    "        |------|-----------|-------------|----------------|\n",
    "        | [Step details] | [Error/No Error] | [Error/No Error] | [No Error/Error/False Positive/False Negative] |\n",
    "\n",
    "        **Summary Statistics:**\n",
    "        - Total correct identifications: [X]/[Total Steps]\n",
    "        - Total false positives: [X]\n",
    "        - Total false negatives: [X]\n",
    "        - Overall accuracy: [X]%\n",
    "\n",
    "        ## Part 2: Error Classification Accuracy\n",
    "        | Step | Benchmark Error Type | AI Error Type | Classification |\n",
    "        |------|---------------------|---------------|----------------|\n",
    "        | [Step with error] | [Error Type] | [Error Type] | [Correct/Incorrect] |\n",
    "\n",
    "        **Summary Statistics:**\n",
    "        - Total correctly classified errors: [X]/[Total Errors]\n",
    "        - Error classification accuracy: [X]%\n",
    "\n",
    "        ## Part 3: Documentation Quality\n",
    "        | Criterion | Rating | Explanation |\n",
    "        |-----------|--------|-------------|\n",
    "        | Structure | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Tense | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Language | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Numbering | [Excellent/Good/Poor] | [Explanation] |\n",
    "        | Timing | [Excellent/Good/Poor] | [Explanation] |\n",
    "\n",
    "        ## Overall Assessment\n",
    "        [Provide a concise overall assessment of the AI documentation's quality, highlighting key strengths and weaknesses, with suggestions for improvement.]\n",
    "\n",
    "        # Input Materials\n",
    "        ## Benchmark Documentation (Ground Truth)\n",
    "        \n",
    "        \"\"\"\n",
    "    ]\n",
    "    inputs.extend(documentation_input)\n",
    "    \n",
    "    inputs.extend([\"## AI-Generated Documentation\"])\n",
    "    inputs.extend(documentation)\n",
    "\n",
    "    evaluation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return evaluation, usage_metadata\n",
    "\n",
    "def get_table_json_prompt(text_with_tables: str, table_identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a prompt to extract a specific table from text into JSON.\n",
    "\n",
    "    Args:\n",
    "        text_with_tables: The full text containing the table(s).\n",
    "        table_identifier: A string to help the model identify the target table\n",
    "                          (e.g., the table title, or a unique phrase near it).\n",
    "\n",
    "    Returns:\n",
    "        A formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data extraction tool.\n",
    "    Your task is to locate a specific table within the provided text and output its data as a JSON array.\n",
    "\n",
    "    Here is the text containing the table(s):\n",
    "    ---TEXT_START---\n",
    "    {text_with_tables}\n",
    "    ---TEXT_END---\n",
    "\n",
    "    Identify the table that best matches the following title: \"{table_identifier}\"\n",
    "\n",
    "    It is very important to you to output the data from ONLY this table as a valid JSON array. Each object in the array should represent a row from the table. The keys of each object should be the exact column headers from the identified table.\n",
    "\n",
    "    Output Constraints:\n",
    "    - Answer direct with the JSON.\n",
    "    - If the specified table cannot be found, output an empty JSON array: []\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_json_from_model_output(model_output_string):\n",
    "    \"\"\"\n",
    "    Extract and parse JSON data from a model output string that contains JSON within code block markers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_output_string : str\n",
    "        The string output from the model that contains JSON within code block markers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe: A pandas DataFrame created from the JSON data, or None if extraction failed\n",
    "    \"\"\"\n",
    "    start_marker = \"```json\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    start_index = model_output_string.find(start_marker)\n",
    "    end_index = model_output_string.find(end_marker, start_index + len(start_marker))  # Search for end marker after the start\n",
    "    \n",
    "    df = None\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        extracted_json_string = model_output_string[start_index + len(start_marker):end_index].strip()\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(extracted_json_string)\n",
    "            logger.info(\"Successfully extracted and parsed JSON.\")\n",
    "            \n",
    "            if isinstance(json_data, list) and all(isinstance(item, dict) for item in json_data):\n",
    "                df = pd.DataFrame(json_data)\n",
    "            else:\n",
    "                logger.warning(\"JSON data is not a list of dictionaries, could not create DataFrame.\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON after extraction: {e}\")\n",
    "            logger.debug(f\"Extracted string: {extracted_json_string}\")\n",
    "    else:\n",
    "        logger.error(\"Could not find JSON code block markers in the output.\")\n",
    "        logger.debug(f\"Model output: {model_output_string}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_table_to_dataframe(evaluation, table_name, model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Extract a table from evaluation content and convert it to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : str\n",
    "        The evaluation content containing tables\n",
    "    table_name : str\n",
    "        The name of the table to extract\n",
    "    model_name : str, optional\n",
    "        The model to use for content generation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature setting for content generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the extracted table data\n",
    "    \"\"\"\n",
    "    # Generate prompt for table extraction\n",
    "    extraction_prompt = get_table_json_prompt(evaluation, table_name)\n",
    "    \n",
    "    # Generate JSON response from the model\n",
    "    json_response, _ = generate_content_from_model(\n",
    "        extraction_prompt,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract and convert JSON to DataFrame\n",
    "    results_df = extract_json_from_model_output(json_response)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_error_evaluation_metrics(evaluation):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive error evaluation metrics from an evaluation document.\n",
    "    \n",
    "    This function extracts tables from the evaluation document and calculates\n",
    "    metrics for error identification, error classification, and documentation quality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    evaluation : str\n",
    "        The evaluation document containing the tables to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing all calculated metrics organized by category\n",
    "    \"\"\"\n",
    "    error_evaluation_metrics = {}\n",
    "    \n",
    "    # Part 1: Error Identification Accuracy\n",
    "    identification_table_name = \"Part 1: Error Identification Accuracy\"\n",
    "    identification_results_df = extract_table_to_dataframe(evaluation, identification_table_name)\n",
    "    \n",
    "    if identification_results_df is not None:\n",
    "        correctly_identified_rows = identification_results_df[\n",
    "            (identification_results_df[\"Classification\"] == \"No Error\") |\n",
    "            (identification_results_df[\"Classification\"] == \"Error (Correctly Identified)\")\n",
    "        ]\n",
    "        total_actual_errors = identification_results_df[identification_results_df[\"Benchmark\"] == \"Error\"]\n",
    "        correctly_identified_errors = identification_results_df[identification_results_df[\"Classification\"] == \"Error (Correctly Identified)\"]\n",
    "        false_positive_errors = identification_results_df[identification_results_df[\"Classification\"] == \"False Positive\"]\n",
    "        false_negative_errors = identification_results_df[identification_results_df[\"Classification\"] == \"False Negative\"]\n",
    "        \n",
    "        error_evaluation_metrics[\"Error Identification Statistics\"] = {\n",
    "            \"Total steps evaluated\": len(identification_results_df),\n",
    "            \"Total correct identifications\": len(correctly_identified_rows),\n",
    "            \"Overall identification accuracy\": len(correctly_identified_rows) / len(identification_results_df) if len(identification_results_df) > 0 else 0,\n",
    "            \"Error recall rate\": len(correctly_identified_errors) / len(total_actual_errors) if len(total_actual_errors) > 0 else \"N/A\",\n",
    "            \"False positive count\": len(false_positive_errors),\n",
    "            \"False negative count\": len(false_negative_errors)\n",
    "        }\n",
    "    else:\n",
    "        error_evaluation_metrics[\"Error Identification Statistics\"] = {\n",
    "            \"Status\": \"No data available\"\n",
    "        }\n",
    "    \n",
    "    # Part 2: Error Classification Accuracy\n",
    "    classification_table_name = \"Part 2: Error Classification Accuracy\"\n",
    "    classification_results_df = extract_table_to_dataframe(evaluation, classification_table_name)\n",
    "    \n",
    "    if classification_results_df is not None:\n",
    "        correctly_classified_errors = classification_results_df[classification_results_df[\"Classification\"] == \"Correct\"]\n",
    "        \n",
    "        error_evaluation_metrics[\"Error Classification Statistics\"] = {\n",
    "            \"Total errors analyzed\": len(classification_results_df),\n",
    "            \"Correctly classified errors\": len(correctly_classified_errors),\n",
    "            \"Classification accuracy\": len(correctly_classified_errors) / len(classification_results_df) if len(classification_results_df) > 0 else 0\n",
    "        }\n",
    "    else:\n",
    "        error_evaluation_metrics[\"Error Classification Statistics\"] = {\n",
    "            \"Status\": \"No data available\"\n",
    "        }\n",
    "    \n",
    "    # # Part 3: Documentation Quality\n",
    "    # documentation_table_name = \"Part 3: Documentation Quality\"\n",
    "    # documentation_quality_df = extract_table_to_dataframe(evaluation, documentation_table_name)\n",
    "\n",
    "    return error_evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(protocol_video_example, protocol_example, lab_video_example, documentation_example,\n",
    "                      protocol_video_input, protocol_input, lab_video_input, \n",
    "                      model_name=\"gemini-2.0-flash\", temperature=0.9):\n",
    "    \"\"\"\n",
    "    Generate corrected documentation by comparing protocol with actual implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    protocol_video_example : list\n",
    "        Example protocol video content\n",
    "    protocol_example : list\n",
    "        Example protocol content\n",
    "    lab_video_example : list\n",
    "        Example lab video content\n",
    "    documentation_example : list\n",
    "        Example documentation content\n",
    "    protocol_video_input : list\n",
    "        Input protocol video content to process\n",
    "    protocol_input : list\n",
    "        Input protocol content to process\n",
    "    lab_video_input : list\n",
    "        Input lab video content to process\n",
    "    model_name : str, optional\n",
    "        The model to use for generation, default is \"gemini-2.0-flash\"\n",
    "    temperature : float, optional\n",
    "        Temperature parameter for generation, default is 0.9\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing the documentation text and usage metadata\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        \"\"\"\n",
    "        You are Professor Matthias Mann, a pioneering scientist in proteomics and mass spectrometry.\n",
    "        # Your Task:\n",
    "        Compare the original protocol with the actual implementation shown in a video, and create a corrected documentation that reflects what actually happened.\n",
    "        Your documentation should follow these guidelines:\n",
    "        1. Keep only relevant sections: Aim, Materials, Procedure, Results\n",
    "        2. Use past tense to describe what actually happened, not what should happen\n",
    "        3. Remove all instructional language and replace with observations\n",
    "        4. Maintain step numbering of the original protocol even if the order is changed (1, 3, 2, ...)\n",
    "        5. Include exact actual timing, not estimated timing\n",
    "        Use these consistent symbols to indicate step status:\n",
    "        - ✓ (Followed correctly with no special notation needed)\n",
    "        - ❌ **Error:** (When something was done incorrectly - be specific about what happened)\n",
    "        - ❌ **Omitted:** (When a step was completely skipped)\n",
    "        - ⚠️ **Deviation:** (When a step was followed differently than prescribed)\n",
    "        - ➕ **Added:** (When a new step not in the protocol was performed)\n",
    "        # Example\n",
    "        \"\"\"\n",
    "    ]\n",
    "    inputs.extend([\"## Protocol video:\"])\n",
    "    inputs.extend(protocol_video_example)\n",
    "    inputs.extend([\"## Protocol:\"])\n",
    "    inputs.extend(protocol_example)\n",
    "    inputs.extend([\"## Lab video:\"])\n",
    "    inputs.extend(lab_video_example)\n",
    "    inputs.extend([\"## Documentation:\"])\n",
    "    inputs.extend(documentation_example)\n",
    "    inputs.extend(\n",
    "        [\"\"\"\n",
    "        # Your task now\n",
    "        Provide me with a documentation as in the example above.\n",
    "        \"\"\"]\n",
    "    )\n",
    "    inputs.extend([\"## Protocol video:\"])\n",
    "    inputs.extend(protocol_video_input)\n",
    "    inputs.extend([\"## Protocol:\"])\n",
    "    inputs.extend(protocol_input)\n",
    "    inputs.extend([\"## Lab video:\"])\n",
    "    inputs.extend(lab_video_input)\n",
    "    inputs.append(\"Output: Correct documentation\")\n",
    "    \n",
    "    documentation, usage_metadata = generate_content_from_model(\n",
    "        inputs,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return documentation, usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed PlaceEvotips_docuCorrect\n",
      "Processed PlaceEvotips_docuWrongPosition\n"
     ]
    }
   ],
   "source": [
    "csv_path = '/Users/patriciaskowronek/Documents/proteomics_specialist/data/benchmark_dataset.csv'\n",
    "protocol_videos_base = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/protocols\"\n",
    "documentation_videos_base = \"/Users/patriciaskowronek/Documents/documentation_agent_few_shot_examples/benchmark_dataset/documentation\"\n",
    "markdown_base = \"/Users/patriciaskowronek/Documents/proteomics_specialist/data\"\n",
    "prefix = \"compare_protocol_video\"\n",
    "\n",
    "all_model_inputs = process_benchmark_dataset(csv_path, protocol_videos_base, documentation_videos_base, markdown_base, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlaceEvotips_docuWrongPosition\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Alright Professor Mann, here's the corrected documentation:\n",
       "\n",
       "## Documentation:# Placing Evotips in Evotip Boxes on the Evosep One System\n",
       "\n",
       "## Aim\n",
       "\n",
       "Placing Evotips in Evotip boxes with HeLa samples and blanks.\n",
       "\n",
       "\n",
       "## Materials\n",
       "\n",
       "### Equipment\n",
       "\n",
       "- **Evotips**\n",
       "  - Single-use stage tips for sample injection\n",
       "  - Rack layout: Two columns (left and right)\n",
       "  - Left column (top to bottom): S1, S2, S3\n",
       "  - Right column (top to bottom): S4, S5, S6\n",
       "  - Within each box: Standard 96-well format with A1 (top left), A12 (top right), H12 (bottom right)\n",
       "- **Evotip Boxes**\n",
       "  - 96-well format (A1-H12) (Figure 1)\n",
       "- **Evosep One System**\n",
       "  - Liquid chromatography system\n",
       "\n",
       "### Reagents\n",
       "\n",
       "- Formic acid (FA)\n",
       "  - ! CAUTION: This liquid may be corrosive. It is harmful and can cause damage if direct contact occurs.\n",
       "\n",
       "### Reagent setup\n",
       "\n",
       "- **Buffer A**\n",
       "  - Consists of 0.1% (vol/vol) FA. The buffers are stable for at least 6 months at room temperature as long as they are protected from sunlight.\n",
       "\n",
       "\n",
       "## Procedure\n",
       "\n",
       "*Estimated timing: less than 1 minute*\n",
       "\n",
       "✓ 1.  Verified that Evotip box was filled to a minimum depth of 1 cm with Buffer A solution. (0:10)\n",
       "\n",
       "✓ 2. Placed Evotip Box at S1 within the rack system of the Evosep instrument. Ensured box was firmly seated in its designated position. (0:23)\n",
       "\n",
       "✓ 3. Placed an empty Evotip Box for Blank tips at S3. Ensured box was firmly seated in its designated position. (0:33)\n",
       "\n",
       "⚠️ 4. Inspected each Evotip before placement to verify its condition. Properly prepared Evotips should display a pale-colored SPE material disc with visible solvent above it. Evotips for HeLa samples were visually confirmed to have a solvent on top and bottom, and the SPE material displayed a pale color (0:47).\n",
       "    - ⚠️ **Deviation:** The \"blanks\" were completely dry, unused tips. (1:22)\n",
       "\n",
       "5. Placed the verified Evotips with HeLa samples into the prepared Evotip boxes at S1 from A1 to A6. (0:50)\n",
       "\n",
       "6. Placed empty Evotips, called Blanks, at S3 from A1 to A6. (1:17)\n",
       "\n",
       "7. Documented the precise position of each placed Evotip. (1:34)\n",
       "\n",
       "\n",
       "## Results\n",
       "- Properly seated Evotip boxes in the rack system\n",
       "- Visible Buffer A solution in boxes (1 cm depth)\n",
       "- All Evotips with HeLa samples showing pale-colored SPE material discs & clear solvent meniscus above each SPE disc of each Evotip.\n",
       "- All Blanks did not contain clear solvent meniscus above each SPE disc of each Evotip.\n",
       "- Evotips with HeLa placed at S1 from A1 to A6.\n",
       "- Blanks placed at S3 from A1 to A6."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Part 1: Error Identification Accuracy\n",
       "\n",
       "| Step | Benchmark | AI Response | Classification |\n",
       "|------|-----------|-------------|----------------|\n",
       "| 1 | No Error | No Error | No Error |\n",
       "| 2 | No Error | No Error | No Error |\n",
       "| 3 | No Error | No Error | No Error |\n",
       "| 4 | No Error | Error | False Positive |\n",
       "| 5 | Error | No Error | False Negative |\n",
       "| 6 | No Error | No Error | No Error |\n",
       "| 7 | No Error | No Error | No Error |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correct identifications: 6/7\n",
       "- Total false positives: 1\n",
       "- Total false negatives: 1\n",
       "- Overall accuracy: 85.7%\n",
       "\n",
       "## Part 2: Error Classification Accuracy\n",
       "\n",
       "| Step | Benchmark Error Type | AI Error Type | Classification |\n",
       "|------|---------------------|---------------|----------------|\n",
       "| 5 | Error | N/A | N/A |\n",
       "\n",
       "**Summary Statistics:**\n",
       "- Total correctly classified errors: 0/0\n",
       "- Error classification accuracy: N/A%\n",
       "\n",
       "## Part 3: Documentation Quality\n",
       "\n",
       "| Criterion | Rating | Explanation |\n",
       "|-----------|--------|-------------|\n",
       "| Structure | Good | The structure is mostly good, but the added sections like \"Reagents\" and the detailed description of \"Equipment\" are unnecessary and detract from the focus on documenting the specific experiment. |\n",
       "| Tense | Excellent | The tense is consistently past tense, accurately reflecting the completed actions. |\n",
       "| Language | Excellent | The language is observational and avoids instructional tone. |\n",
       "| Numbering | Excellent | The original step numbering is maintained correctly. |\n",
       "| Timing | Excellent | The AI includes exact timing for each step, which is a significant improvement. |\n",
       "\n",
       "## Overall Assessment\n",
       "\n",
       "The AI documentation demonstrates a good understanding of the required format and successfully converts the protocol into an observational document. The use of past tense, observational language, and accurate timing are strengths. However, the AI incorrectly identifies an error in step 4 and misses the error in step 5, and introduces irrelevant details in the \"Materials\" section. To improve, the AI should focus on accurately identifying errors and avoiding the inclusion of extraneous information not directly related to the observed experiment.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 10:50:46,868 - __main__ - INFO - Successfully extracted and parsed JSON.\n",
      "2025-04-21 10:50:47,470 - __main__ - INFO - Successfully extracted and parsed JSON.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Error Classification Statistics': {'Classification accuracy': 0.0,\n",
      "                                     'Correctly classified errors': 0,\n",
      "                                     'Total errors analyzed': 1},\n",
      " 'Error Identification Statistics': {'Error recall rate': 0.0,\n",
      "                                     'False negative count': 1,\n",
      "                                     'False positive count': 1,\n",
      "                                     'Overall identification accuracy': 0.7142857142857143,\n",
      "                                     'Total correct identifications': 5,\n",
      "                                     'Total steps evaluated': 7}}\n"
     ]
    }
   ],
   "source": [
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "example = 'PlaceEvotips_docuCorrect'\n",
    "protocol_video_example = all_model_inputs[example]['protocol_video_input']\n",
    "protocol_example = all_model_inputs[example]['protocol_input']\n",
    "lab_video_example = all_model_inputs[example]['lab_video_input']\n",
    "documentation_example = all_model_inputs[example]['documentation_input']\n",
    "\n",
    "copy_all_model_inputs = all_model_inputs.copy()\n",
    "subset_all_model_inputs = copy_all_model_inputs.pop('PlaceEvotips_docuCorrect')\n",
    "results_collection = {}\n",
    "for key, value in copy_all_model_inputs.items():\n",
    "    print(key)\n",
    "    protocol_video_input = value['protocol_video_input']\n",
    "    protocol_input = value['protocol_input']\n",
    "    lab_video_input = value['lab_video_input']\n",
    "    documentation_input = value['documentation_input']\n",
    "\n",
    "    documentation, usage_metadata = generate_documentation(\n",
    "        protocol_video_example, protocol_example, lab_video_example, documentation_example,\n",
    "        protocol_video_input, protocol_input, lab_video_input, \n",
    "        model_name=\"gemini-2.0-flash\", \n",
    "        temperature=0.9\n",
    "    )\n",
    "    display(Markdown(documentation))\n",
    "\n",
    "    evaluation, usage_metadata_evaluation = generate_documentation_evaluation(documentation_input, documentation)\n",
    "    display(Markdown(evaluation))\n",
    "\n",
    "    metrics = calculate_error_evaluation_metrics(evaluation)\n",
    "    pprint.pprint(metrics)\n",
    "\n",
    "    results_collection[key] = {\n",
    "        \"inputs\": {\n",
    "            \"experiment_name\": key,\n",
    "            \"protocol_video_input\": value['protocol_video_input'],\n",
    "            \"protocol_input\": value['protocol_input'],\n",
    "            \"lab_video_input\": value['lab_video_input'],\n",
    "            \"documentation_input\": value['documentation_input']\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"documentation\": documentation,\n",
    "            \"documentation_metadata\": usage_metadata,\n",
    "            \"evaluation\": evaluation,\n",
    "            \"evaluation_metadata\": usage_metadata_evaluation,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
